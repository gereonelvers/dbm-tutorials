<!DOCTYPE html>
<!-- _layouts/distill.html --><html>

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Neural Keyword Spotting on LibriBrain | Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</title>
    <meta name="author" content="  ">
    <meta name="description" content="End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.">
    <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/tutorials/assets/img/brain-icon.svg">
    
    <link rel="stylesheet" href="/tutorials/assets/css/main.css">
    <link rel="canonical" href="https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/tutorials/assets/js/theme.js"></script>
    <script src="/tutorials/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/tutorials/assets/js/distillpub/template.v2.js"></script>
    <script src="/tutorials/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/tutorials/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body class="fixed-top-nav">

    <d-front-matter>
      <!--- <script async type="text/json">{ -->
      <script type="text/json">{
        "title": "Neural Keyword Spotting on LibriBrain",
        "description": "End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.",
        "published": "November 24, 2025",
        "authors": [
          {
            "author": "Gereon Elvers","authorURL": "https://gereonelvers.com","affiliations": [
              {
                "name": "PNPL, University of Oxford"}
            ]},
          {
            "author": "Gilad Landau","affiliations": [
              {
                "name": "PNPL, University of Oxford"}
            ]},
          {
            "author": "Oiwi Parker Jones","affiliations": [
              {
                "name": "PNPL, University of Oxford"}
            ]}
          
        ],
        "katex": {
          "delimiters": [
            {
              "left": "$",
              "right": "$",
              "display": false
            },
            {
              "left": "$$",
              "right": "$$",
              "display": true
            }
          ]
        }
      }</script>
    </d-front-matter>

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/tutorials/">Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <!-- [#&lt;Jekyll::Page @relative_path=&quot;404.html&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/about.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/call.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;index.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;assets/css/main.scss&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;robots.txt&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/submitting.md&quot;&gt;, #&lt;Jekyll:Archive @type=year @title={:year=&gt;&quot;2025&quot;} @data={&quot;layout&quot;=&gt;&quot;archive-year&quot;}&gt;, #&lt;JekyllRedirectFrom::PageWithoutAFile @relative_path=&quot;redirects.json&quot;&gt;, #&lt;JekyllFeed::PageWithoutAFile @relative_path=&quot;feed.xml&quot;&gt;, #&lt;Jekyll::PaginateV2::Generator::PaginationPage @relative_path=&quot;.html&quot;&gt;, #&lt;Jekyll::PageWithoutAFile @relative_path=&quot;sitemap.xml&quot;&gt;] -->

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/tutorials/2023/about">about</a>
              </li> -->
              <!-- Call -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/tutorials/2023/call">call</a>
              </li> -->
              <!-- submissions -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/tutorials/2023/submissions/">submissions</a>
              </li> -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/tutorials/blog/">blog</a>
              </li> -->
              <!-- 2022 -->
              <!-- <li class="nav-item">
                <a class="nav-link" href="https://iclr-blog-track.github.io/home/">2022 edition <u>⤤</u></a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/tutorials/about/">about</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/tutorials/call/">call for tutorial</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/tutorials/submitting/">submitting</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/tutorials/blog/index.html">blog</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Neural Keyword Spotting on LibriBrain</h1>
        <p>End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#introduction">Introduction</a></div>
            <div><a href="#motivation-and-context">Motivation and Context</a></div>
            <div><a href="#dataset-and-methodology">Dataset and Methodology</a></div>
            <ul>
              <li><a href="#the-libribrain-dataset">The LibriBrain Dataset</a></li>
              <li><a href="#task-formulation">Task Formulation</a></li>
              <li><a href="#model-architecture">Model Architecture</a></li>
              <li><a href="#training-strategy">Training Strategy</a></li>
              
            </ul>
<div><a href="#evaluation-metrics">Evaluation Metrics</a></div>
            <div><a href="#computational-requirements">Computational Requirements</a></div>
            <div><a href="#learning-goals">Learning Goals</a></div>
            <div><a href="#tutorial-structure">Tutorial Structure</a></div>
            <div><a href="#notebook-access">Notebook Access</a></div>
            
          </nav>
        </d-contents>

        <h2 id="introduction">Introduction</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-480.webp 480w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-800.webp 800w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
      
    
    <img src="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
    Overview of the neural keyword spotting task: A participant listens to audiobook speech while MEG sensors record brain activity. The system processes neural signals to detect when specific keywords (like "Watson") are heard, producing a probability score for each word window.
</div>

<blockquote>
  <p><strong>Note</strong>: This tutorial is released in conjunction with our DBM workshop paper <em>“Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset”</em><d-cite key="elvers2025elementary"></d-cite>. The tutorial provides a comprehensive introduction as well as a hands-on, pedagogical walkthrough of the methods and concepts presented in the paper.</p>
</blockquote>

<p><strong>Neural Keyword Spotting (KWS)</strong> from brain signals presents a promising direction for non-invasive brain–computer interfaces (BCIs), with potential applications in assistive communication technologies for individuals with speech impairments. While invasive BCIs have achieved remarkable success in speech decoding<d-cite key="willett2023speech,metzger2023neuroprosthesis"></d-cite>, non-invasive approaches using magnetoencephalography (MEG) or electroencephalography (EEG) remain challenging due to lower signal-to-noise ratios and the difficulty of detecting brief, rare events in continuous neural recordings.</p>

<p>This tutorial demonstrates how to build and evaluate a neural keyword spotting system using the <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite>—a large-scale MEG corpus with over 50 hours of recordings from a single participant listening to audiobooks. We focus on the practical challenges of extreme class imbalance, appropriate evaluation metrics, and techniques for training models that can distinguish keyword occurrences from the continuous stream of speech.</p>

<h2 id="motivation-and-context">Motivation and Context</h2>

<h3 id="why-keyword-spotting">Why Keyword Spotting?</h3>

<p>Full speech decoding from non-invasive brain signals remains an open challenge. However, <strong>keyword spotting</strong>—detecting specific words of interest—offers a more tractable goal that could still enable meaningful communication. Even detecting a single keyword reliably (a “1-bit channel”) could significantly improve quality of life for individuals with severe communication disabilities, allowing them to:</p>

<ul>
  <li>Answer yes/no questions</li>
  <li>Signal alerts or requests</li>
  <li>Control devices through specific command words</li>
  <li>Maintain basic communication when other channels fail</li>
</ul>

<h3 id="the-challenge-rare-events-in-noisy-data">The Challenge: Rare Events in Noisy Data</h3>

<p>Keyword spotting from MEG presents two fundamental challenges:</p>

<ol>
  <li>
    <p><strong>Extreme Class Imbalance</strong>: Even short, common words like “the” represent only ~5.5% of all words in naturalistic speech. Target keywords like “Watson” appear in just 0.12% of word windows, creating a severe imbalance.</p>
  </li>
  <li>
    <p><strong>Low Signal-to-Noise Ratio</strong>: Unlike invasive recordings with electrode arrays placed directly on the cortex, non-invasive MEG/EEG sensors sit outside the skull, capturing attenuated and spatially blurred neural signals mixed with physiological and environmental noise.</p>
  </li>
</ol>

<p>These challenges require specialized techniques, which we cover in this tutorial.</p>

<h2 id="dataset-and-methodology">Dataset and Methodology</h2>

<h3 id="the-libribrain-dataset">The LibriBrain Dataset</h3>

<p>The <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite> is a publicly available MEG corpus featuring over 50 hours of continuous recordings from a single participant listening to Sherlock Holmes audiobooks. The dataset is released as a set of preprocessed HDF5 files with word- and phoneme-level event annotation for each session, collected using a MEGIN Triux™ Neo system. The dimension of the currently released data is 306 sensor channels x 250 Hz.</p>

<h3 id="task-formulation">Task Formulation</h3>

<p>We frame keyword detection as <strong>event-referenced binary classification</strong>:</p>

<ul>
  <li>
<strong>Input</strong>: MEG signals (306 channels × T timepoints) windowed around word onsets</li>
  <li>
<strong>Output</strong>: Probability p ∈ [0, 1] that the target keyword occurs in this window</li>
  <li>
<strong>Window length</strong>: Keyword duration + buffers (pre-/post-onset)</li>
</ul>

<p>This differs from continuous detection by:</p>
<ol>
  <li>Focusing on word boundaries (where linguistic information peaks)</li>
  <li>Avoiding the combinatorial explosion of sliding windows</li>
  <li>Leveraging precise temporal alignment from annotations</li>
</ol>

<p><strong>Data Splits</strong>: We use multiple training sessions and dynamically select validation/test sessions based on keyword prevalence to ensure sufficient positive examples in held-out sets.</p>

<h3 id="model-architecture">Model Architecture</h3>

<p>The tutorials baseline model addresses the challenges through three components:</p>

<blockquote>
  <p><strong>Note</strong>: The notebook first demonstrates individual components with simplified examples (e.g., <code class="language-plaintext highlighter-rouge">ConvTrunk</code> with stride-2), then presents the full training architecture below.</p>
</blockquote>

<h4 id="1-convolutional-trunk">1. Convolutional Trunk</h4>
<p>The model begins with a Conv1D layer projecting the 306 MEG channels to 128 dimensions, followed by a residual block<d-cite key="he2016deep"></d-cite>. A key design choice is <strong>aggressive temporal downsampling</strong>: a stride-25 convolution with kernel size 50 reduces the sequence length by ~25× while expanding the receptive field. Two additional Conv1D layers refine the 128-dimensional representation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">306</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">ResNetBlock1D</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># stride-25 downsampling
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="2-temporal-attention">2. Temporal Attention</h4>
<p>The trunk output is projected to 512 dimensions before splitting into two parallel 1×1 convolution heads: one producing per-timepoint logits, the other producing attention scores. The attention mechanism<d-cite key="ilse2018attention"></d-cite> learns to focus on brief, informative time windows (e.g., around keyword onsets) while down-weighting noise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">logits_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">attn_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">trunk</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">logit_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">logits_t</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_t</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">logit_t</span> <span class="o">*</span> <span class="n">attn</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="3-loss-functions-for-extreme-imbalance">3. Loss Functions for Extreme Imbalance</h4>
<p>Standard cross-entropy fails under extreme class imbalance. We employ two complementary losses:</p>

<ul>
  <li>
    <p><strong>Focal Loss</strong><d-cite key="lin2017focal"></d-cite>: Down-weights easy negatives by $(1-p_t)^\gamma$, with class prior $\alpha=0.95$ matching the &lt;1% base rate. This prevents “always negative” collapse.</p>
  </li>
  <li>
    <p><strong>Pairwise Ranking Loss</strong><d-cite key="burges2010ranknet"></d-cite>: Directly optimizes the ordering of positive vs. negative scores, improving precision-recall trade-offs:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pairwise_logistic_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">pos_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">1</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="n">neg_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="c1"># Sample pairs and penalize inversions
</span>    <span class="n">margins</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">pos_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">scores</span><span class="p">[</span><span class="n">sampled_neg_idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">margins</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="training-strategy">Training Strategy</h3>

<p><strong>Balanced Sampling</strong>: We construct training batches with ~10% positive rate (vs. natural &lt;1%) by:</p>
<ol>
  <li>Including most/all positive examples</li>
  <li>Subsampling negatives proportionally</li>
  <li>Shuffling each batch</li>
</ol>

<p>This ensures gradients aren’t starved by all-negative batches while keeping evaluation on natural class priors for realistic metrics.</p>

<p><strong>Preprocessing</strong>: The dataset applies per-channel z-score normalization and clips outliers beyond ±10σ before feeding data to the model.</p>

<p><strong>Data Augmentation</strong><d-cite key="buda2018systematic"></d-cite> (applied during training only):</p>
<ul>
  <li>Temporal shifts: randomly roll each sample by ±4 timepoints (±16ms at 250 Hz)</li>
  <li>Additive Gaussian noise: σ=0.01 added to normalized signals</li>
</ul>

<p><strong>Regularization</strong>: Dropout (p=0.5), weight decay<d-cite key="loshchilov2019decoupled"></d-cite> (1e-4), and early stopping on validation loss.</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<p>Traditional accuracy is meaningless under extreme imbalance (always predicting “no keyword” yields &gt;99% accuracy). We employ metrics that reflect real-world BCI deployment:</p>

<h3 id="threshold-free-metrics">Threshold-Free Metrics</h3>

<p><strong>Area Under Precision-Recall Curve (AUPRC)</strong><d-cite key="saito2015precision,davis2006relationship"></d-cite>:</p>
<ul>
  <li>Baseline equals positive class prevalence (~0.001 for “Watson” on the full dataset, ~0.005 on the test set chosen to maximize prevalence)</li>
  <li>Aim for 2–10× improvement over chance</li>
  <li>More informative than AUROC under heavy imbalance</li>
</ul>

<p><strong>Precision-Recall Trade-off</strong>:</p>
<ul>
  <li>
<strong>Precision</strong>: Fraction of predicted keywords that are correct (controls false alarms)</li>
  <li>
<strong>Recall</strong>: Fraction of true keywords detected</li>
</ul>

<h3 id="user-facing-deployment-metrics">User-Facing Deployment Metrics</h3>

<p><strong>False Alarms per Hour (FA/h)</strong>:</p>
<ul>
  <li>Practical constraint: target &lt;10 FA/h for usability</li>
  <li>Computed as: <code class="language-plaintext highlighter-rouge">(False Positives / total_seconds) × 3600</code>
</li>
  <li>Evaluated at fixed recall targets (e.g., 0.2, 0.4, 0.6)</li>
</ul>

<p><strong>Operating Point Selection</strong>:
Choose threshold on validation to meet FA/h or precision targets; report test results at that fixed threshold.</p>

<h3 id="performance-interpretation">Performance Interpretation</h3>

<ul>
  <li>
<strong>Chance</strong>: Prevalence (% of words matching the keyword)</li>
  <li>
<strong>2–5× Chance</strong>: Modest but meaningful improvement</li>
  <li>
<strong>&gt;10× Chance</strong>: Strong performance for this challenging task</li>
</ul>

<h2 id="computational-requirements">Computational Requirements</h2>
<ul>
  <li>
<strong>GPU</strong>: Google Colab free tier (T4/L4 GPU) sufficient</li>
  <li>
<strong>Training Time</strong>: ~30 minutes for the baseline on default configuration</li>
  <li>
<strong>Memory</strong>: &lt;16 GB GPU RAM with batch size 64</li>
  <li>
<strong>Dataset</strong>: Automatically downloaded by the <code class="language-plaintext highlighter-rouge">pnpl</code> library (~50 GB for the full set, ~5GB for the default subset)</li>
</ul>

<p>The tutorial is designed to run on consumer hardware by training on a subset of data. To scale to the full 50+ hours of data, increase training sessions in the configuration and use a higher-tier GPU (V100/A100).</p>

<h2 id="learning-goals">Learning Goals</h2>

<p>By working through this tutorial, you will:</p>

<ol>
  <li>
<strong>Frame KWS from continuous MEG</strong> as a rare-event detection problem with event-referenced windowing</li>
  <li>
<strong>Handle extreme class imbalance</strong> through balanced sampling, focal loss, and pairwise ranking</li>
  <li>
<strong>Build a lightweight temporal model</strong> (Conv1D + attention) trainable on consumer GPUs</li>
  <li>
<strong>Evaluate with appropriate metrics</strong>: AUPRC, FA/h at fixed recall, precision-recall curves</li>
  <li>
<strong>Understand trade-offs</strong> between sensitivity (recall), false alarm rate, and practical usability</li>
  <li>
<strong>Gain hands-on experience</strong> with a real-world non-invasive BCI dataset</li>
</ol>

<h2 id="tutorial-structure">Tutorial Structure</h2>

<p>The accompanying Jupyter notebook provides a complete, executable walkthrough:</p>

<ol>
  <li>
<strong>Setup &amp; Configuration</strong> — Install dependencies, configure paths and hyperparameters</li>
  <li>
<strong>Dataset Exploration</strong> — Inspect HDF5 files (MEG signals) and TSV files (annotations)</li>
  <li>
<strong>Problem Formulation</strong> — Visualize challenges (class imbalance, signal noise)</li>
  <li>
<strong>Model Components</strong> — Interactive demos of each architectural component:
    <ul>
      <li>Convolutional trunk (spatial-temporal processing)</li>
      <li>Temporal attention (adaptive pooling)</li>
      <li>Focal loss (imbalance handling)</li>
      <li>Pairwise ranking (order-based training)</li>
      <li>Balanced sampling (batch composition)</li>
    </ul>
  </li>
  <li>
<strong>Training</strong> — Full training loop with PyTorch Lightning, early stopping, logging</li>
  <li>
<strong>Evaluation</strong> — AUPRC, ROC, FA/h curves, confusion matrices, threshold analysis</li>
  <li>
<strong>Next Steps</strong> — Suggested experiments (different keywords, architectures, augmentations)</li>
</ol>

<h2 id="notebook-access">Notebook Access</h2>

<p>Access the full interactive tutorial:</p>

<div style="display: flex; gap: 10px; margin: 20px 0;">
  <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank" rel="external nofollow noopener noopener noreferrer">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
  </a>
  <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank" rel="external nofollow noopener noopener noreferrer">
    <img src="https://img.shields.io/badge/GitHub-View%20Source-blue?logo=github" alt="View on GitHub">
  </a>
</div>

<p><strong>Links</strong>:</p>
<ul>
  <li>
<strong>Interactive (Colab)</strong>: <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">Open in Google Colab</a>
</li>
  <li>
<strong>Source (GitHub)</strong>: <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">View on GitHub</a>
</li>
  <li>
<strong>Workshop Paper</strong>: <a href="https://arxiv.org/abs/2510.21038" rel="external nofollow noopener noopener noreferrer" target="_blank">arXiv:2510.21038</a>
</li>
  <li>
<strong>LibriBrain Dataset</strong>: <a href="https://huggingface.co/datasets/pnpl/LibriBrain" rel="external nofollow noopener noopener noreferrer" target="_blank">View on HuggingFace</a>
</li>
</ul>

<p><strong>Requirements</strong>: A Google account for Colab, or local Jupyter Notebook install with Python 3.10+</p>

<hr>

<p>Besides the accompanying workshop paper <d-cite key="elvers2025elementary"></d-cite>, this tutorial builds on work from the 2025 LibriBrain Competition<d-cite key="landau2025pnpl"></d-cite> centered around the LibriBrain dataset<d-cite key="ozdogan2025libribrain"></d-cite>. These papers contain more comprehensive bibliographies which might be helpful for readers seeking additional context.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <d-bibliography src="/tutorials/assets/bibliography/2025-11-24-neural-keyword-spotting-on-libribrain.bib"></d-bibliography>

    <d-article id="bibtex-container" class="related highlight">
      <p>For attribution in academic contexts, please cite this work as</p>
      <pre id="bibtex-academic-attribution">PLACEHOLDER FOR ACADEMIC ATTRIBUTION</pre>

      <p>BibTeX citation</p>
      <pre id="bibtex-box">PLACEHOLDER FOR BIBTEX</pre>
    </d-article>


    <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async>
</script>

    <script>
      // Wait for Distill.js to process everything
      window.addEventListener('load', () => {
        let trimIt = (e) => e.trim();
        let getText = (e) => e.innerText;
        let splitNameAndFamilyName = (e) => {
          let splitted = e.split(" ");

          let fnames = splitted.slice(0, -1).join(" ");
          let lname = splitted.at(-1);

          return [lname, fnames];
        }

        let authors = Array.from(document.getElementsByClassName("author")).map(getText).map(trimIt).map(splitNameAndFamilyName);

        // Check if authors exist
        if (!authors || authors.length === 0) {
          console.warn("No authors found - citation generation skipped");
          return;
        }

        let firstAuthorLName = authors[0][0];
        let affiliationElements = Array.from(document.getElementsByClassName("affiliation")).filter(e => e.nodeName === "P").map(getText).map(trimIt);

        // getting stuff directly from Jekyll
        let publishedWhen = "November 24, 2025";
        let title = "Neural Keyword Spotting on LibriBrain";
        let description = "End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.";

        {
          let authorsBibtex = authors.map(e => `${e[0]}, ${e[1]}`).join(" and ");
          let bibtexTitleShorthand = (firstAuthorLName +
                  "2025" +
                  title.split(" ").slice(0, 3).join("")
          ).replace(" ", "").replace(/[\p{P}$+<=>^`|~]/gu, '').toLowerCase().trim();
          let bibtexTemplate = `
@inproceedings{${bibtexTitleShorthand},
  author = {${authorsBibtex}},
  title = {${title}},
  abstract = {${description}},
  booktitle = {NeurIPS 2025 Workshop on "Data on the Brain \\& Mind" Tutorials Track},
  year = {2025},
  date = {${publishedWhen}},
  note = {${window.location.href}},
  url  = {${window.location.href}}
}
  `.trim();
          document.getElementById("bibtex-box").innerText = bibtexTemplate;
        }

        {
          let academicLFI = authors.map(e => e[0]);
          {
            if (academicLFI.length > 2) academicLFI = academicLFI[0] + ", et al.";
            else if (academicLFI.length == 2) academicLFI = academicLFI[0] + " & " + academicLFI[1];
            else academicLFI = academicLFI[0];
          }
          let academicTemplate = `
${academicLFI}, "${title}", NeurIPS 2025 Workshop on "Data on the Brain & Mind" Tutorials Track, 2025.
`.trim();
          document.getElementById("bibtex-academic-attribution").innerText = academicTemplate;
        }
      });
    </script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  </body>


</html>
