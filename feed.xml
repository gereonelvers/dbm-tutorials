<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://data-brain-mind.github.io/tutorials/feed.xml" rel="self" type="application/atom+xml" /><link href="https://data-brain-mind.github.io/tutorials/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-24T07:35:31+08:00</updated><id>https://data-brain-mind.github.io/tutorials/feed.xml</id><title type="html">Data on the Brain &amp;amp; Mind Tutorial Track (NeurIPS 2025)</title><subtitle>Home to the Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)
</subtitle><entry><title type="html">Accelerated Methods in {Multi-Modal, Multi-Metric, Many-Model} CogNeuroAI</title><link href="https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/" rel="alternate" type="text/html" title="Accelerated Methods in {Multi-Modal, Multi-Metric, Many-Model} CogNeuroAI" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/"><![CDATA[<p>In this tutorial, we cover a series of modern computational modeling methods in CogNeuroAI (the emergent, interdisciplinary intersection of Cognitive Science, Neuroscience, and AI), with a particular focus on accelerating research in representational alignment.</p>

<p>The tutorial is written end-to-end as an interactive Jupyter notebook, and is designed not only to provide a comprehensive and generally accessible methodological overview to audience members of diverse backgrounds, but also to provide access to highly-optimized software that will allow participants to quickly and readily adapt the tutorial’s methods to their own research.</p>

<p>The tutorial is subdivided in multiple related, but otherwise containerized parts, and includes:</p>
<ul>
  <li>Introduction to DeepJuice: A custom, highly-optimized, end-to-end GPU-accelerated library designed to facilitate high-throughput brain &amp; behavioral modeling at scale</li>
  <li>The step-by-step reproduction of a recent large-scale, controlled model comparison pipeline on the 7T fMRI Natural Scenes Dataset</li>
  <li>A case study of cross-modal representational alignment (using language models to predict image-evoked brain activity in human vental visual cortex)</li>
  <li>A demonstration of hypothesis-driven, vector-semantic mapping: a technique (derived from relative representation / anchor-point embedding analysis) that allows researchers to query the underlying structure of representational alignment with natural language.</li>
</ul>

<p>The tutorial code is available at the following anonymous GitHub link: https://anonymous.4open.science/r/DBM-Tutorial-CFC4/DBM2025.ipynb</p>]]></content><author><name>Colin Conwell</name></author><summary type="html"><![CDATA[A tutorial showcasing a number of (GPU-accelerated) methods for probing the representational alignment of brains, minds, and machines.]]></summary></entry><entry><title type="html">An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute</title><link href="https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/" rel="alternate" type="text/html" title="An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/"><![CDATA[<p>The following tutorial will provide a brief introduction to this dataset then demonstrate how to load an example session and do some simple analysis. We’ll cover the following topics:</p>

<ul>
  <li>Introduction to the experiment</li>
  <li>Accessing the dataset</li>
  <li>Reading the project metadata</li>
  <li>Loading an experiment for analysis</li>
  <li>Plotting neural activity aligned to stimuli</li>
  <li>Using optotagging to infer cell type</li>
  <li>Accessing LFP data</li>
  <li>Analyzing behavioral data during the visual change detection task</li>
</ul>

<h2 id="introduction-to-the-experiment">Introduction to the experiment</h2>

<p>Our ability to perceive the sensory environment and flexibly interact with the world requires the coordinated action of neuronal populations distributed throughout the brain. To further our understanding of the neural basis of behavior, the Visual Behavior project leveraged the Allen Brain Observatory pipeline (diagrammed below; gray panels refer to the companion <a href="https://portal.brain-map.org/circuits-behavior/visual-behavior-2p">Visual Behavior Optical Physiology dataset</a>) to collect a large-scale, highly standardized dataset consisting of recordings of neural activity in mice that have learned to perform a visual chang detection task. This dataset can be used to investigate how patterns of spiking activity across the visual cortex and thalamus are related to behavior and also how these activity dynamics are influenced by task-engagement and prior visual experience. </p>

<p>The Visual Behavior Neuropixels dataset includes 153 sessions from 81 mice. These data are made openly accessible, with all recorded timeseries, behavioral events, and experimental metadata conveniently packaged in Neurodata Without Borders (NWB) files that can be accessed and analyzed using our open Python software package, the  <a href="https://github.com/AllenInstitute/AllenSDK">AllenSDK</a>.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="the-visual-change-detection-task">The visual change detection task</h3>

<p>The Visual Behavior Optical Physiology and Visual Behavior Neuropixels projects are built upon a change detection behavioral task. Briefly, in this go/no-go task, mice are shown a continuous series of briefly presented visual images and they earn water rewards by correctly reporting when the identity of the image changes (diagrammed below). Five percent of images are omitted, allowing for analysis of expectation signals.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="neuropixels-recordings-throughout-visual-cortex-and-thalamus">Neuropixels recordings throughout visual cortex and thalamus</h3>

<p>This dataset includes multi-regional Neuropixels recordings from up to 6 probes at once. The probes target six visual cortical areas including VISp, VISl, VISal, VISrl, VISam, and VISpm. In addition, multiple subcortical areas are also typically measured, including visual thalamic areas LGd and LP as well as units in the hippocampus and midbrain. Note that for the first release, NWB files will include spike times for units recorded in these structures, but LFP data will not be available.</p>

<p>Recordings were made in three genotypes: C57BL6J, Sst-IRES-Cre; Ai32, and Vip-IRES-Cre; Ai32. By crossing Sst and Vip lines to the Ai32 ChR2 reporter mouse, we were able to activate putative Sst+ and Vip+ cortical interneurons by stimulating the cortical surface with blue light during an optotagging protocol at the end of each session.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="investigating-the-impact-of-stimulus-novelty-on-neural-responses-and-behavior">Investigating the impact of stimulus novelty on neural responses and behavior</h3>

<p>To allow analysis of stimulus novelty on neural responses and behavior, two different images sets were used in the recording sessions: G and H (diagrammed below). Both image sets comprised 8 natural images. Two images were shared across the two image sets (purple in diagram), enabling within session analysis of novelty effects. Mice took one of the following three trajectories through training and the two days of recording:</p>

<p>1) Train on G; see G on the first recording day; see H on the second recording day</p>

<p>2) Train on G; see H on the first recording day; see G on the second recording day</p>

<p>3) Train on H; see H on the first recording day; see G on the second recording day</p>

<p>The numbers in the <em>Training and Recording Workflow</em> bubble below give the total recording sessions of each type in the dataset.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><a href="https://anonymous.4open.science/r/neurips-vbn-tutorial-7B18/vbn_neurips_tutorial.ipynb">Tutorial link</a></p>]]></content><author><name>Corbett Bennett</name></author><summary type="html"><![CDATA[Tutorial on analyzing the Visual Behavior Neuropixels dataset from the Allen Institute]]></summary></entry><entry><title type="html">CONFORM: A Project to Create Crowd-Sourced Open Neuroscience fMRI Foundation Models</title><link href="https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/" rel="alternate" type="text/html" title="CONFORM: A Project to Create Crowd-Sourced Open Neuroscience fMRI Foundation Models" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/"><![CDATA[<h2 id="background-and-introduction">Background and Introduction</h2>

<p>Creating a foundational human fMRI model is a critical next step for extending modern NeuroAI<d-cite key="gifford2024opportunities"></d-cite>. To achieve this, the model must generalize across both individuals and tasks, which requires a large volume of data with many participants, observations, and diverse stimuli.</p>

<p>Historically, a significant impediment has been that most fMRI studies have small sample sizes and a low number of observations per session; the latter also leading to poor stimulus diversity. As a result, typical fMRI experiments sample only a tiny fraction of the human population and the vast space of real-world visual, auditory, or linguistic inputs. These limitations impeded efforts to draw robust conclusions from fMRI data and to integrate insights from modern AI systems into our understanding of the human brain—a challenge that is exacerbated by the inherently noisy BOLD signal.</p>

<p>In visual neuroscience, a first step in meeting this challenge has already been taken through the collection of large-scale fMRI datasets, which typically include brain responses from a small number of participants each scanned over many repeated sessions (15-40 hours-long sessions), who view a large number of stimuli (5000-10,000 stimuli per participant)<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023"></d-cite>. This approach of “deeply sampling” a small number of participants increases the statistical power of experiments <d-cite key="NASELARIS2021sampling,baker2021power"></d-cite>, and enables powerful parameter-rich, within-subject models. While this approach of collecting large datasets from small groups of participants has led to hundreds of publications and impactful discoveries, even this strategy is neither sustainable nor scalable for both scientific and practical reasons:</p>

<ol>
  <li>
    <p><strong>Participant burden and attrition</strong>: Successful data collection at this scale depends on <em>heroic</em> efforts by both experimenters and participants. The time commitment and scheduling complexities are onerous: participants, experimenters, and scanners must remain consistently healthy and available (e.g., in both the BOLD5000 and NSD datasets at least one participant failed to complete the study<d-cite key="chang2019bold5000,allen2022massive"></d-cite>; in the THINGS dataset one participant was canceled due to “technical issues”<d-cite key="things2023"></d-cite>).</p>
  </li>
  <li>
    <p><strong>Limited sample size</strong>: Even with this extraordinary amount of effort, data was collected from only 3-8 participants—a small number that does not support the hoped-for population diversity expected of human neural foundation models.</p>
  </li>
  <li>
    <p><strong>Constrained stimulus diversity</strong>: Stimulus diversity is necessarily limited by small participant pools and the need for stimulus repeats and/or overlap across participants<d-cite key="Prince2022"></d-cite>. Even within a single recurring participant, only a limited number of observations are possible. Moreover, controlled tasks and stimulus selection methods have further reduced diversity in the visual images included in each dataset: NSD uses only COCO images (only 80 object categories<d-cite key="lin2014microsoft"></d-cite>, which leave gaps in many regions of natural image space<d-cite key="rothsample"></d-cite>), BOLD5000 uses COCO as well as SUN<d-cite key="SUN_Xiao2010"></d-cite> and ImageNet<d-cite key="russakovsky2015imagenet"></d-cite> images, and THINGS uses a larger number of “concepts”, but depicted as single cropped objects that show little context<d-cite key="things2023"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Infrastructure challenges</strong>: Creating the infrastructure for data management and distribution is a considerable technical challenge. Short-term it requires a robust and replicable data processing pipeline and a reliable platform for data distribution. Long-term it requires stability—years later the distribution website should remain readily accessible.</p>
  </li>
  <li>
    <p><strong>Financial barriers</strong>: The monetary costs of collecting data can present a challenge to any single lab (e.g., five participants across 25 × one hour scans could easily cost on the order of $100,000) and risks over-representing the interests of the small number of labs with the necessary resources.</p>
  </li>
</ol>

<p>Despite their increased scale relative to standard fMRI studies, these datasets still present significant challenges in the construction of NeuroAI models. The number of observations and participants is still small for purposes of model training, and data quality is dependent on preprocessing methods. More importantly, prediction accuracy and decoding performance are typically high only when trained and tested within the same participant—due to inherent structural and functional differences between individual brains and, at present, weak methods for generalizing across them. Consequently, when models are applied across participants, even within the same study, their performance and decoding capabilities decrease dramatically.</p>

<p><img src="/tutorials/assets/img/2025-11-24-conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/Workshop-NeurIPS25.png" alt="CONFORM workflow" /></p>

<p><strong>Figure 1: CONFORM workflow.</strong> A single, optimized experimental design is distributed to multiple sites for data collection. The collected data is then centralized for preprocessing, alignment, and integration into a foundational dataset. This process creates a continuous feedback loop, allowing the dataset to grow in size and diversity, which informs future experimental design and provides the basis for a strong foundation model.</p>

<h2 id="towards-a-dynamic-foundation-model-for-visual-fmri">Towards a Dynamic Foundation Model for Visual fMRI</h2>

<p>We propose <strong>CONFORM</strong> (Crowd-Sourced Open Neuroscience fMRI Foundation Model)—a strategy for building foundational human visual fMRI models through community-contributed datasets and models. Following previous efforts in systems neuroscience <d-cite key="internationalbrain2017laboratory"></d-cite>, we propose to leverage multi-site crowd-sourcing to enable collection of larger and more diverse datasets, along with new computational advances to facilitate coherent analysis. As detailed below, the building blocks of CONFORM are already in place, spanning four key domains:</p>

<ol>
  <li>
    <p><strong>A larger-scale and highly diverse dataset</strong> that aggregates close to 100 participants and 100,000s of natural scenes depicting 1000’s of object categories/concepts in context. “MOSAIC”<d-cite key="lahner_mosaic_2025"></d-cite> is a scalable framework for combining extant fMRI datasets<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023,lahner_modeling_2024,zhou2023large,shen2019deep,horikawa2017generic"></d-cite>, using common preprocessing and registration, into a single, extremely large-scale and extensible vision dataset. MOSAIC Repository: <a href="https://registry.opendata.aws/mosaic/">https://registry.opendata.aws/mosaic/</a></p>
  </li>
  <li>
    <p><strong>Higher data quality</strong> through an enhanced preprocessing pipeline to improve the signal-to-noise ratio of measured BOLD responses. Building on <em>GLMSingle</em><d-cite key="Prince2022"></d-cite> and Generative Modeling of Signal and Noise (<em>GSN</em><d-cite key="kay2025disentangling"></d-cite>), we are developing <em>PSN</em> (Partitioning of Signal and Noise)—a powerful low-rank denoising method that optimally separates signal from noise in neural data, outperforming trial-averaging and PCA, especially when noise is structured or complex (as in fMRI). PSN Repository: <a href="https://github.com/jacob-prince/PSN">https://github.com/jacob-prince/PSN</a></p>
  </li>
  <li>
    <p><strong>Enhanced generalization</strong> to new participants from outside-of-dataset studies using “BraInCoRL”—a meta-learned in-context foundation model that enables generalization using only a small amount of additional data<d-cite key="yu2025-braInCoRL"></d-cite>. BrainCoRL Repository: <a href="https://github.com/leomqyu/BraInCoRL">https://github.com/leomqyu/BraInCoRL</a></p>
  </li>
  <li>
    <p><strong>Crowd-sourcing infrastructure</strong> to support the continuous integration of data from new studies across unique participants and data collection sites.</p>
  </li>
</ol>

<p>Building on these methodological advances and the lessons learned from distributed large-scale fMRI datasets<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023,lahner_modeling_2024"></d-cite>, CONFORM will be a unique collaborative modeling strategy that will enable the creation of large-scale vision foundation fMRI models on datasets with improved signal quality, more participants, greater stimulus diversity, and which, critically, generalizes to new participants and studies in low data regimes. Longer term—across labs, participants, and MRI systems, we further propose a “crowd-sourced” community-driven effort to collect and integrate new data, thereby continuously improving the models. Given the challenges of collecting ever-larger and more diverse datasets at a single site, we suggest that crowd-sourcing is the only tenable solution for building appropriate-scale, truly foundational neural datasets. However, developing a viable crowd-sourcing infrastructure at this scale remains an unsolved challenge with a very high risk/reward tradeoff.</p>

<p>We are taking on this challenge by integrating and further developing recent advances in fMRI preprocessing, data aggregation, and generalization. CONFORM will also include the infrastructure for continuously expanding the dataset’s size and the diversity of its stimuli<d-cite key="wang2022incorporating"></d-cite>. Our project will use a two-pronged approach for data contributions: locally directed and globally directed.</p>

<p>The <strong>locally directed</strong> model is straightforward: the CONFORM distribution website will also accept contributions. In contrast to other neural data repositories<d-cite key="markiewicz2021openneuro"></d-cite>, we will provide detailed specifications for the acceptable designs, stimuli, tasks, and data formats to ensure submissions can be seamlessly integrated into CONFORM with high data quality. One attractive aspect of a locally directed model is that CONFORM may be able to re-purpose extant data that was already collected for a different purpose, thus giving new life to data that may have been otherwise dormant for years. At the same time, processing all available public data is not feasible. As an alternative, we will facilitate researchers re-analyzing their datasets with our pipeline. Our goal with the locally directed model is to be as inclusive as possible with stimuli and tasks, even with necessary limitations.</p>

<p>The <strong>globally directed</strong> model is more ambitious and forward-looking, and offers a greater potential payoff. We will provide a complete, turn-key study design to participating research sites, streamlining the data collection process. We will optimize the selection of stimulus images to achieve the best possible distribution of images within natural image space across many participants<d-cite key="rothsample"></d-cite>. We will also optimize for repeated stimuli and partial stimulus overlap across the population. Similarly, we will optimize the study design with respect to scanning parameters and trial structure. Collaborators will be able to specify both the length of scan sessions and the total number of participants they contribute. They will then be provided with complete scan protocols, experimental control files, and stimulus images. An interface on the same website used for distribution will allow them to download these files and upload their collected data for incorporation into the dataset.</p>

<p>CONFORM’s framework towards a scalable foundation fMRI model will enable powerful insights into human vision. Datasets within CONFORM will continue to grow in size and stimulus diversity as the community contributes data. Critically, the resultant models will achieve improved generalization to new participants across diverse subpopulations, requiring only a relatively small amount of data per individual. As such, CONFORM will dramatically broaden the accessibility of NeuroAI methods, empowering researchers in a much wider range of scientific domains to make new discoveries.</p>

<h3 id="improving-data-qualitypsn">Improving Data Quality—PSN</h3>

<p>The recently introduced <em>GLMSingle</em> preprocessing pipeline dramatically improves the signal-to-noise ratio of measured BOLD responses acquired using standard fMRI methods<d-cite key="Prince2022"></d-cite>. In parallel, the Generative Modeling of Signal and Noise technique (<em>GSN</em><d-cite key="kay2025disentangling"></d-cite>) has established a new paradigm for accurately estimating the parameters of the signal and noise distributions that give rise to the observed measurements. We are building upon the GLMSingle and GSN approaches in developing <em>PSN</em> (Partitioning of Signal and Noise)—a low-rank denoising method that optimally separates signal from noise in neural data, improving the performance and interpretability of downstream computational models.</p>

<p>PSN addresses a core challenge in building a truly foundational fMRI dataset by maximizing the amount of stimulus-driven information (signal) that can be recovered from each participant’s measurements, while partitioning out the influence of other sources of variability (noise). Conventional denoising strategies such as trial averaging are straightforward and widely used, but they rely on the assumptions that noise is independent across trials and uncorrelated between voxels. In actuality, these assumptions are often violated in fMRI data, where noise can be structured, spatially correlated, and non-stationary. Similarly, PCA-based low-rank denoising identifies directions of highest variance but does not explicitly distinguish between signal and noise, leading to bias when noise variance is large or when signal and noise share overlapping subspaces<d-cite key="kay2025disentangling,pospisil2024revisiting"></d-cite>.</p>

<p>PSN addresses these limitations by extending the GSN framework<d-cite key="kay2025disentangling"></d-cite> to produce denoised trial-averaged data that are optimized for downstream modeling. GSN first estimates separate covariance structures for the signal and noise directly from repeated-trial measurements. These estimates define a signal-aware basis for low-rank reconstruction, allowing us to then selectively preserve dimensions most likely to reflect stimulus-driven activity while discarding those dominated by noise.</p>

<p>Critically, PSN relies on cross-validation to determine the optimal number of signal dimensions to retain, with thresholds chosen either at the multi-voxel or single-voxel level, depending on the data’s heterogeneity in feature tuning and signal-to-noise ratio. This cross-validated tailoring of denoising parameters will be particularly important given CONFORM’s aim of integrating large, multi-site datasets, where measurement quality can vary widely across participants, scanners, and brain regions.</p>

<p>In simulations with known ground truth, PSN consistently recovers more accurate signal estimates than trial averaging or PCA-based methods, achieving lower variance without introducing substantial bias. Applied to real datasets, including primate electrophysiology and human fMRI, PSN yields substantial gains in cross-validated encoding model performance and improves the interpretability of model-derived feature visualization (manuscript in preparation). In the context of CONFORM, applying PSN to every contributed dataset ensures that all data entering the foundation model are maximally informative, optimized for data quality and reliability, and robust to the structured noise sources inherent in large-scale, crowd-sourced fMRI. Finally, because non-stimulus-driven sources of neural variability may themselves be of scientific interest, PSN also enables these components to be cleanly separated for downstream analyses that focus on modeling noise rather than signal.</p>

<h3 id="integration-of-fmri-data-across-studiesmosaic">Integration of fMRI Data Across Studies—MOSAIC</h3>

<p>Individual fMRI experiments face practical constraints that create trade-offs between the number of participants, the number of experimental trials, and stimulus diversity. Any resulting conclusions are thus limited in scope. However, the aggregation of existing fMRI datasets, here called MOSAIC (Meta-Organized Stimuli And fMRI Imaging data for Computational modeling), achieves a vastly larger scale useful for measuring cross-dataset and cross-subject generalization and training of high-parameter artificial neural networks.</p>

<p>MOSAIC<d-cite key="lahner_mosaic_2025"></d-cite> currently preprocesses eight event-related fMRI vision datasets (Natural Scenes Dataset<d-cite key="allen2022massive"></d-cite>, Natural Object Dataset<d-cite key="gong_large-scale_2023"></d-cite>, BOLD Moments Dataset<d-cite key="lahner_modeling_2024"></d-cite>, BOLD5000<d-cite key="chang2019bold5000"></d-cite>, Human Actions Dataset, Deeprecon<d-cite key="shen2019deep"></d-cite>, Generic Object Decoding<d-cite key="horikawa2017generic"></d-cite>, and THINGS<d-cite key="things2023"></d-cite>) with a shared pipeline and registers all data to the same cortical surface space. Single-trial beta values in MOSAIC are estimated using GLMsingle and a high integrity test-train split is curated across datasets.</p>

<p>At present, MOSAIC contains 430,007 fMRI-stimulus pairs from 93 participants across 162,839 unique image stimuli. The stimuli are further divided into 144,360 training stimuli, 18,145 test stimuli, and 334 synthetic stimuli for rigorous model training and evaluation. Their shared preprocessing pipeline uses open source frameworks and is thus compatible with methods advancements such as PSN and expansion to other registration spaces such as subject native. Crucial to CONFORM, datasets can be added to MOSAIC <em>post-hoc</em> regardless of experimental design, acquisition, and size.</p>

<p>MOSAIC is a critical first step to enable researchers to overcome individual dataset limitations and tackle complex research questions at an unprecedented scale. The MOSAIC dataset and preprocessing code will be available soon for download. In tandem with the MOSAIC team, the larger CONFORM community will work to leverage MOSAIC’s extensible design to allow the seamless integration of new datasets, creating an evolving foundation for collaborative human vision research.</p>

<h3 id="generalizing-across-participants-and-studies-in-a-low-data-regimebraincorl">Generalizing Across Participants and Studies in a Low Data Regime—BraInCoRL</h3>

<p>Different datasets may utilize different stimuli, employ different scanning parameters, and collect data from diverse populations. This makes it challenging to build generalizable models that predict neural activity across diverse participants. Traditional approaches require large, participant-specific fMRI datasets, limiting their scalability for clinical and research applications. This variability in cortical organization—driven by anatomical and functional differences, developmental experiences, and learning—necessitates a framework that can adapt to new individuals with minimal data while capturing shared functional principles of visual processing.</p>

<p>To address this, BrainCoRL (Brain In-Context Representation Learning)<d-cite key="yu2025-braInCoRL"></d-cite> leverages meta-learning and transformer-based in-context learning to predict voxelwise neural responses from few-shot examples without fine-tuning. Inspired by how large language models adapt to new tasks through contextual examples, BrainCoRL treats each voxel’s response function as a learnable mapping that can be inferred from limited data. The model is trained across multiple participants to discover shared functional principles of visual processing, enabling it to rapidly adapt to new individuals without additional fine-tuning.</p>

<p>BrainCoRL outperforms traditional voxelwise encoding models in low-data regimes, generalizes to entirely new fMRI datasets acquired with different scanners and protocols, and provides interpretable insights into cortical selectivity through its attention mechanisms. Notably, the framework can also link neural responses to natural language descriptions, opening new possibilities for query-driven functional mapping of the visual cortex. By dramatically reducing the data requirements for accurate neural encoding models, this work paves the way for more scalable and personalized applications in both basic neuroscience and clinical settings, where understanding individual differences in brain organization is crucial for diagnosis and treatment.</p>

<h2 id="impact-and-conclusions">Impact and Conclusions</h2>

<p>Although existing large-scale fMRI datasets have been valuable, used in hundreds of studies to support a wide range of novel scientific discoveries, they are limited by their single-site, small-N approach. To move beyond this, we propose CONFORM—a unique crowd-sourcing strategy that leverages recent advances in data processing, data aggregation, analysis, and a new crowd-sourced infrastructure. This new approach directly addresses the financial and logistical challenges of collecting large datasets while enabling unprecedented stimulus diversity. However, simply crowd-sourcing data is not enough; CONFORM’s success will be predicated on the specific data and modeling optimizations we introduce to handle the multifaceted noise inherent in fMRI. Moreover, by creating models that can effectively predict new data with only a small amount of information, we will dramatically broaden the accessibility of NeuroAI methods. This will empower a much wider range of researchers to leverage the power of modern AI using the typical scale of data they collect, ultimately accelerating scientific discovery.</p>

<p>Critically, generalizing across individuals requires addressing both biological differences and technical noise sources, such as artifacts from different scanners and motion. We directly tackle these challenges through a three-pronged approach:</p>

<ol>
  <li>
    <p><strong>Data Acquisition</strong>: Collect a limited amount of data from each participant, including repeated and partially overlapping stimuli across the population, to boost both data quality and stimulus diversity.</p>
  </li>
  <li>
    <p><strong>Denoising</strong>: Apply a two-level denoising strategy. Use GLMsingle to optimize the signal-to-noise ratio within each subject and, then, apply PSN to separate stimulus-related variance from idiosyncratic noise, improving data quality and interpretability.</p>
  </li>
  <li>
    <p><strong>Alignment</strong>: Learn a mapping from the denoised data into a shared representational space, thereby allowing us to make accurate predictions across individuals. This can be achieved through advanced methods such as BrainCoRL, which does not require overlapping stimuli, or using standard functional alignment techniques that rely on overlapping stimuli in the denoised data.</p>
  </li>
</ol>

<p>By integrating and advancing these tools to create a true foundational model, we can answer downstream questions using the dataset population to make predictions about new individuals or clinical populations. For example, recent advances in visualizing and labeling neural representations of object categories<d-cite key="luo2023brain,luo2024brainscuba"></d-cite> could be extended to autistic individuals, thereby providing a much clearer picture of the encoding of atypically processed visual information (e.g., human faces). Thus, a wide range of research domains will have access to modern AI methods using only the scale of data they typically collect. Ultimately, this generalizability will enable the next generation of insights into brain function across a much wider range of populations.</p>]]></content><author><name>Michael J. Tarr</name></author><summary type="html"><![CDATA[CONFORM is a crowd-sourced framework to build a human fMRI foundation model through improved preprocessing, dataset aggregation, and in-context representation learning.]]></summary></entry><entry><title type="html">Neural Keyword Spotting on LibriBrain</title><link href="https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain/" rel="alternate" type="text/html" title="Neural Keyword Spotting on LibriBrain" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain/"><![CDATA[<h2 id="introduction">Introduction</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-480.webp 480w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-800.webp 800w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
    Overview of the neural keyword spotting task: A participant listens to audiobook speech while MEG sensors record brain activity. The system processes neural signals to detect when specific keywords (like "Watson") are heard, producing a probability score for each word window.
</div>

<blockquote>
  <p><strong>Note</strong>: This tutorial is released in conjunction with our DBM workshop paper <em>“Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset”</em><d-cite key="elvers2025elementary"></d-cite>. The tutorial provides a comprehensive introduction as well as a hands-on, pedagogical walkthrough of the methods and concepts presented in the paper.</p>
</blockquote>

<p><strong>Neural Keyword Spotting (KWS)</strong> from brain signals presents a promising direction for non-invasive brain–computer interfaces (BCIs), with potential applications in assistive communication technologies for individuals with speech impairments. While invasive BCIs have achieved remarkable success in speech decoding<d-cite key="willett2023speech,metzger2023neuroprosthesis"></d-cite>, non-invasive approaches using magnetoencephalography (MEG) or electroencephalography (EEG) remain challenging due to lower signal-to-noise ratios and the difficulty of detecting brief, rare events in continuous neural recordings.</p>

<p>This tutorial demonstrates how to build and evaluate a neural keyword spotting system using the <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite>—a large-scale MEG corpus with over 50 hours of recordings from a single participant listening to audiobooks. We focus on the practical challenges of extreme class imbalance, appropriate evaluation metrics, and techniques for training models that can distinguish keyword occurrences from the continuous stream of speech.</p>

<h2 id="motivation-and-context">Motivation and Context</h2>

<h3 id="why-keyword-spotting">Why Keyword Spotting?</h3>

<p>Full speech decoding from non-invasive brain signals remains an open challenge. However, <strong>keyword spotting</strong>—detecting specific words of interest—offers a more tractable goal that could still enable meaningful communication. Even detecting a single keyword reliably (a “1-bit channel”) could significantly improve quality of life for individuals with severe communication disabilities, allowing them to:</p>

<ul>
  <li>Answer yes/no questions</li>
  <li>Signal alerts or requests</li>
  <li>Control devices through specific command words</li>
  <li>Maintain basic communication when other channels fail</li>
</ul>

<h3 id="the-challenge-rare-events-in-noisy-data">The Challenge: Rare Events in Noisy Data</h3>

<p>Keyword spotting from MEG presents two fundamental challenges:</p>

<ol>
  <li>
    <p><strong>Extreme Class Imbalance</strong>: Even short, common words like “the” represent only ~5.5% of all words in naturalistic speech. Target keywords like “Watson” appear in just 0.12% of word windows, creating a severe imbalance.</p>
  </li>
  <li>
    <p><strong>Low Signal-to-Noise Ratio</strong>: Unlike invasive recordings with electrode arrays placed directly on the cortex, non-invasive MEG/EEG sensors sit outside the skull, capturing attenuated and spatially blurred neural signals mixed with physiological and environmental noise.</p>
  </li>
</ol>

<p>These challenges require specialized techniques, which we cover in this tutorial.</p>

<h2 id="dataset-and-methodology">Dataset and Methodology</h2>

<h3 id="the-libribrain-dataset">The LibriBrain Dataset</h3>

<p>The <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite> is a publicly available MEG corpus featuring over 50 hours of continuous recordings from a single participant listening to Sherlock Holmes audiobooks. The dataset is released as a set of preprocessed HDF5 files with word- and phoneme-level event annotation for each session, collected using a MEGIN Triux™ Neo system. The dimension of the currently released data is 306 sensor channels x 250 Hz.</p>

<h3 id="task-formulation">Task Formulation</h3>

<p>We frame keyword detection as <strong>event-referenced binary classification</strong>:</p>

<ul>
  <li><strong>Input</strong>: MEG signals (306 channels × T timepoints) windowed around word onsets</li>
  <li><strong>Output</strong>: Probability p ∈ [0, 1] that the target keyword occurs in this window</li>
  <li><strong>Window length</strong>: Keyword duration + buffers (pre-/post-onset)</li>
</ul>

<p>This differs from continuous detection by:</p>
<ol>
  <li>Focusing on word boundaries (where linguistic information peaks)</li>
  <li>Avoiding the combinatorial explosion of sliding windows</li>
  <li>Leveraging precise temporal alignment from annotations</li>
</ol>

<p><strong>Data Splits</strong>: We use multiple training sessions and dynamically select validation/test sessions based on keyword prevalence to ensure sufficient positive examples in held-out sets.</p>

<h3 id="model-architecture">Model Architecture</h3>

<p>The tutorials baseline model addresses the challenges through three components:</p>

<blockquote>
  <p><strong>Note</strong>: The notebook first demonstrates individual components with simplified examples (e.g., <code class="language-plaintext highlighter-rouge">ConvTrunk</code> with stride-2), then presents the full training architecture below.</p>
</blockquote>

<h4 id="1-convolutional-trunk">1. Convolutional Trunk</h4>
<p>The model begins with a Conv1D layer projecting the 306 MEG channels to 128 dimensions, followed by a residual block<d-cite key="he2016deep"></d-cite>. A key design choice is <strong>aggressive temporal downsampling</strong>: a stride-25 convolution with kernel size 50 reduces the sequence length by ~25× while expanding the receptive field. Two additional Conv1D layers refine the 128-dimensional representation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">306</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">ResNetBlock1D</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># stride-25 downsampling
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="2-temporal-attention">2. Temporal Attention</h4>
<p>The trunk output is projected to 512 dimensions before splitting into two parallel 1×1 convolution heads: one producing per-timepoint logits, the other producing attention scores. The attention mechanism<d-cite key="ilse2018attention"></d-cite> learns to focus on brief, informative time windows (e.g., around keyword onsets) while down-weighting noise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">logits_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">attn_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">trunk</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">logit_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">logits_t</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_t</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">logit_t</span> <span class="o">*</span> <span class="n">attn</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="3-loss-functions-for-extreme-imbalance">3. Loss Functions for Extreme Imbalance</h4>
<p>Standard cross-entropy fails under extreme class imbalance. We employ two complementary losses:</p>

<ul>
  <li>
    <p><strong>Focal Loss</strong><d-cite key="lin2017focal"></d-cite>: Down-weights easy negatives by $(1-p_t)^\gamma$, with class prior $\alpha=0.95$ matching the &lt;1% base rate. This prevents “always negative” collapse.</p>
  </li>
  <li>
    <p><strong>Pairwise Ranking Loss</strong><d-cite key="burges2010ranknet"></d-cite>: Directly optimizes the ordering of positive vs. negative scores, improving precision-recall trade-offs:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pairwise_logistic_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">pos_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">1</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="n">neg_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="c1"># Sample pairs and penalize inversions
</span>    <span class="n">margins</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">pos_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">scores</span><span class="p">[</span><span class="n">sampled_neg_idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">margins</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="training-strategy">Training Strategy</h3>

<p><strong>Balanced Sampling</strong>: We construct training batches with ~10% positive rate (vs. natural &lt;1%) by:</p>
<ol>
  <li>Including most/all positive examples</li>
  <li>Subsampling negatives proportionally</li>
  <li>Shuffling each batch</li>
</ol>

<p>This ensures gradients aren’t starved by all-negative batches while keeping evaluation on natural class priors for realistic metrics.</p>

<p><strong>Preprocessing</strong>: The dataset applies per-channel z-score normalization and clips outliers beyond ±10σ before feeding data to the model.</p>

<p><strong>Data Augmentation</strong><d-cite key="buda2018systematic"></d-cite> (applied during training only):</p>
<ul>
  <li>Temporal shifts: randomly roll each sample by ±4 timepoints (±16ms at 250 Hz)</li>
  <li>Additive Gaussian noise: σ=0.01 added to normalized signals</li>
</ul>

<p><strong>Regularization</strong>: Dropout (p=0.5), weight decay<d-cite key="loshchilov2019decoupled"></d-cite> (1e-4), and early stopping on validation loss.</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<p>Traditional accuracy is meaningless under extreme imbalance (always predicting “no keyword” yields &gt;99% accuracy). We employ metrics that reflect real-world BCI deployment:</p>

<h3 id="threshold-free-metrics">Threshold-Free Metrics</h3>

<p><strong>Area Under Precision-Recall Curve (AUPRC)</strong><d-cite key="saito2015precision,davis2006relationship"></d-cite>:</p>
<ul>
  <li>Baseline equals positive class prevalence (~0.001 for “Watson” on the full dataset, ~0.005 on the test set chosen to maximize prevalence)</li>
  <li>Aim for 2–10× improvement over chance</li>
  <li>More informative than AUROC under heavy imbalance</li>
</ul>

<p><strong>Precision-Recall Trade-off</strong>:</p>
<ul>
  <li><strong>Precision</strong>: Fraction of predicted keywords that are correct (controls false alarms)</li>
  <li><strong>Recall</strong>: Fraction of true keywords detected</li>
</ul>

<h3 id="user-facing-deployment-metrics">User-Facing Deployment Metrics</h3>

<p><strong>False Alarms per Hour (FA/h)</strong>:</p>
<ul>
  <li>Practical constraint: target &lt;10 FA/h for usability</li>
  <li>Computed as: <code class="language-plaintext highlighter-rouge">(False Positives / total_seconds) × 3600</code></li>
  <li>Evaluated at fixed recall targets (e.g., 0.2, 0.4, 0.6)</li>
</ul>

<p><strong>Operating Point Selection</strong>:
Choose threshold on validation to meet FA/h or precision targets; report test results at that fixed threshold.</p>

<h3 id="performance-interpretation">Performance Interpretation</h3>

<ul>
  <li><strong>Chance</strong>: Prevalence (% of words matching the keyword)</li>
  <li><strong>2–5× Chance</strong>: Modest but meaningful improvement</li>
  <li><strong>&gt;10× Chance</strong>: Strong performance for this challenging task</li>
</ul>

<h2 id="computational-requirements">Computational Requirements</h2>
<ul>
  <li><strong>GPU</strong>: Google Colab free tier (T4/L4 GPU) sufficient</li>
  <li><strong>Training Time</strong>: ~30 minutes for the baseline on default configuration</li>
  <li><strong>Memory</strong>: &lt;16 GB GPU RAM with batch size 64</li>
  <li><strong>Dataset</strong>: Automatically downloaded by the <code class="language-plaintext highlighter-rouge">pnpl</code> library (~50 GB for the full set, ~5GB for the default subset)</li>
</ul>

<p>The tutorial is designed to run on consumer hardware by training on a subset of data. To scale to the full 50+ hours of data, increase training sessions in the configuration and use a higher-tier GPU (V100/A100).</p>

<h2 id="learning-goals">Learning Goals</h2>

<p>By working through this tutorial, you will:</p>

<ol>
  <li><strong>Frame KWS from continuous MEG</strong> as a rare-event detection problem with event-referenced windowing</li>
  <li><strong>Handle extreme class imbalance</strong> through balanced sampling, focal loss, and pairwise ranking</li>
  <li><strong>Build a lightweight temporal model</strong> (Conv1D + attention) trainable on consumer GPUs</li>
  <li><strong>Evaluate with appropriate metrics</strong>: AUPRC, FA/h at fixed recall, precision-recall curves</li>
  <li><strong>Understand trade-offs</strong> between sensitivity (recall), false alarm rate, and practical usability</li>
  <li><strong>Gain hands-on experience</strong> with a real-world non-invasive BCI dataset</li>
</ol>

<h2 id="tutorial-structure">Tutorial Structure</h2>

<p>The accompanying Jupyter notebook provides a complete, executable walkthrough:</p>

<ol>
  <li><strong>Setup &amp; Configuration</strong> — Install dependencies, configure paths and hyperparameters</li>
  <li><strong>Dataset Exploration</strong> — Inspect HDF5 files (MEG signals) and TSV files (annotations)</li>
  <li><strong>Problem Formulation</strong> — Visualize challenges (class imbalance, signal noise)</li>
  <li><strong>Model Components</strong> — Interactive demos of each architectural component:
    <ul>
      <li>Convolutional trunk (spatial-temporal processing)</li>
      <li>Temporal attention (adaptive pooling)</li>
      <li>Focal loss (imbalance handling)</li>
      <li>Pairwise ranking (order-based training)</li>
      <li>Balanced sampling (batch composition)</li>
    </ul>
  </li>
  <li><strong>Training</strong> — Full training loop with PyTorch Lightning, early stopping, logging</li>
  <li><strong>Evaluation</strong> — AUPRC, ROC, FA/h curves, confusion matrices, threshold analysis</li>
  <li><strong>Next Steps</strong> — Suggested experiments (different keywords, architectures, augmentations)</li>
</ol>

<h2 id="notebook-access">Notebook Access</h2>

<p>Access the full interactive tutorial:</p>

<div style="display: flex; gap: 10px; margin: 20px 0;">
  <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
  </a>
  <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-View%20Source-blue?logo=github" alt="View on GitHub" />
  </a>
</div>

<p><strong>Links</strong>:</p>
<ul>
  <li><strong>Interactive (Colab)</strong>: <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb">Open in Google Colab</a></li>
  <li><strong>Source (GitHub)</strong>: <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb">View on GitHub</a></li>
  <li><strong>Workshop Paper</strong>: <a href="https://arxiv.org/abs/2510.21038">arXiv:2510.21038</a></li>
  <li><strong>LibriBrain Dataset</strong>: <a href="https://huggingface.co/datasets/pnpl/LibriBrain">View on HuggingFace</a></li>
</ul>

<p><strong>Requirements</strong>: A Google account for Colab, or local Jupyter Notebook install with Python 3.10+</p>

<hr />

<p>Besides the accompanying workshop paper <d-cite key="elvers2025elementary"></d-cite>, this tutorial builds on work from the 2025 LibriBrain Competition<d-cite key="landau2025pnpl"></d-cite> centered around the LibriBrain dataset<d-cite key="ozdogan2025libribrain"></d-cite>. These papers contain more comprehensive bibliographies which might be helpful for readers seeking additional context.</p>]]></content><author><name>Gereon Elvers</name></author><summary type="html"><![CDATA[End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.]]></summary></entry><entry><title type="html">NLDisco: A Pipeline for Interpretable Neural Latent Discovery</title><link href="https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery/" rel="alternate" type="text/html" title="NLDisco: A Pipeline for Interpretable Neural Latent Discovery" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery/"><![CDATA[<p>This tutorial introduces <strong>NLDisco</strong> (<strong>N</strong>eural <strong>L</strong>atent <strong>Disco</strong>very pipeline), a machine learning approach for discovering interpretable features in high-dimensional neural recordings. The method leverages sparse autoencoders (SAEs) to identify meaningful latent dimensions that correspond to specific behavioural or environmental variables.</p>

<p><strong>What you’ll accomplish:</strong></p>
<ul>
  <li>Train sparse autoencoders on neural spike data to automatically discover interpretable features</li>
  <li>Use Matryoshka architecture to capture both broad and specific neural patterns simultaneously</li>
  <li>Validate discoveries through an interactive dashboard that visualizes feature-behavior relationships</li>
</ul>

<p><strong>Code:</strong> <a href="https://anonymous.4open.science/r/nldisco-CB03/notebooks/NLDisco_tutorial.ipynb">NLDisco Tutorial Notebook</a></p>]]></content><author><name>Anaya Gaelle Pouget</name></author><summary type="html"><![CDATA[This tutorial introduces NLDisco (Neural Latent Discovery pipeline), a machine learning approach for discovering interpretable features in high-dimensional neural recordings. The method leverages sparse autoencoders (SAEs) to identify meaningful latent dimensions that correspond to specific behavioural or environmental variables.]]></summary></entry><entry><title type="html">Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations</title><link href="https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/" rel="alternate" type="text/html" title="Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/"><![CDATA[<h2 id="introduction-and-motivation">Introduction and motivation</h2>
<h3 id="neural-decoding-models-from-science-fiction-to-reality">Neural decoding models: from science fiction to reality</h3>
<p>Many of us have seen movies or read stories where people can “mind read,” such as Professor X, an exceptionally powerful telepath who can read and control the thoughts of others. While this belongs to science fiction, advances in technology and computation are bringing us closer to decoding aspects of human thought in real life. Neural decoding models powered by modern natural language processing and large language models have begun to approximate how humans process and generate language. These models, in turn, have helped researchers to understand how human brains accomplish the same tasks.</p>
<h3 id="simultaneous-eeg-and-eye-tracking-for-cognitive-dataset">Simultaneous EEG and eye-tracking for cognitive dataset</h3>
<p>Building and refining powerful neural decoding models require large-scale and high-quality cognitive data. A powerful way to capture such cognitive data is to combine eye-tracking with electroencephalography (EEG). Eye-tracking captures gaze positions, which link visual input to specific task stimuli or words at given time. EEG, meanwhile, provides non-invasive and relatively low-cost recordings of brain dynamics with high temporal resolution.</p>
<h3 id="what-are-some-challenges">What are some challenges?</h3>
<h4 id="the-rarity-of-datasets-on-natural-reading">The rarity of datasets on natural reading</h4>
<p>Most existing simultaneous EEG and eye-tracking datasets focus on non-linguistic tasks. For example, EEGEyeNet provides large-scale recordings of eye movements while participants view simple visual symbols <d-cite key="Kastrati2021"></d-cite>, and EEGT-RSOD <d-cite key="He2025"></d-cite> records brain and eye activity as participants search for targets in remote sensing images. For reading specifically, the most well-known datasets are ZuCo <d-cite key="Hollenstein2018"></d-cite> and ZuCo 2.0 <d-cite key="Hollenstein2019"></d-cite>, which combine EEG and eye-tracking during sentence-level reading (i.e., each stimulus is a single sentence). These natural (like) reading resources have become popular and invaluable for advancing EEG-to-text decoding and related computational models.</p>
<h4 id="frequent-mind-wandering-off-task-thoughts-during-reading">Frequent mind-wandering (off-task thoughts) during reading</h4>
<p>You might argue that existing datasets could be sufficient for neural decoding tasks if we apply data augmentation techniques. But there’s another, even more fundamental challenge: human attention as a confunding factor. Just as “Attention is all you need” in computational models <d-cite key="Vaswani2017"></d-cite>, attention is also critical in humans. Our attention is not static: it fluctuates with arousal, fatigue, mood, and motivation <d-cite key="Shen2024, Smallwood2009"></d-cite>, and these states alter how we engage with language and modulate behaviors and neural activity <d-cite key="Smallwood2011, Unsworth2013"></d-cite>. One particularly common and impactful attention state is <strong>mind-wandering (MW)</strong>, when our focus drifts from the task at hand to unrelated thoughts. <strong>People spend 30% to 60% of their daily lives mind-wandering</strong> <d-cite key="Killingsworth2010"></d-cite>, and it happens frequently during reading <d-cite key="Smallwood2011"></d-cite>. You might even find yourself mind-wandering while reading this blog post. While the exact cognitive processes behind MW are still unclear, one leading hypothesis, the perceptual decoupling hypothesis <d-cite key="Smallwood2006"></d-cite>, suggests that internal thoughts during MW divert resources away from external stimuli. This diversion can directly affect how language is processed. For these reasons, accounting for MW is not just interesting; it is essential for building models that can mechanistically and predictively capture real human language processing.</p>
<h3 id="mind-wandering-is-sneaky-so-how-do-we-study-it">Mind-wandering is sneaky, so how do we study it?</h3>
<h4 id="previous-approaches-and-their-drawbacks">Previous approaches and their drawbacks</h4>
<p>Now let’s take a brief detour into psychology. Studying constructs like MW can include unique challenges. Prior research has primarily relied on self-reports and thought probes to detect episodes of MW. Self-reports depend on participants’ subjective awareness: whenever individuals realize that their minds have drifted from the task, they are instructed to report it <d-cite key="Schooler2002"></d-cite>. Thought probes, by contrast, are random prompts that require participants to indicate whether they are currently on-task or mind-wandering <d-cite key="Giambra1995, Smallwood2006"></d-cite>. While both approaches are widely used in contexts such as reading <d-cite key="Reichle2010, Broadway2015, Faber2017"></d-cite>, they only mark when MW ends, making it difficult to capture its onset or to characterize how episodes naturally unfold over time, temporal features that are crucial for understanding and detecting MW.</p>
<h4 id="the-remind-paradigm">The ReMind paradigm</h4>
<p>So how did we solve this problem? I spent days reading research articles, and my own wandering mind made me reread passages over and over just to comprehend the material. Suddenly, it hit me: when we reread after getting lost, we are implicitly marking the parts of the text where our attention drifted. That observation inspired the ReMind paradigm, which estimates the onset and duration of MW episodes during reading by combining retrospective self-reports with eye-tracking. Participants indicate the words where they believe their mind started and stopped wandering for each episode. We then align these selections with gaze timestamps to estimate precise onset and offset times.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/intro_pic.png" alt="intro_pic" />
	<em>Figure created by Sora.</em></p>

<h3 id="introduce-roamm-dataset">Introduce ROAMM dataset</h3>
<p>Using this approach, we created the Reading Observed at Mindless Moments (ROAMM) dataset, which contains simultaneous EEG and eye-tracking data from 44 participants (30 females, 9 males, and 5 non-binary) performing naturalistic reading in the ReMind paradigm. The dataset includes rich labels, such as the word at each fixation, attention state at each sample, and a comprehension question score for each page. By capturing attention states in a naturalistic and uninterrupted way, ROAMM provides a powerful resource for studying questions in language and cognition, and for developing language-based machine learning models that better reflect real human attention.</p>

<p>In the remainder of the post, we present the ROAMM Dataset including the participant demographics, task, data acquisition and pre-processing, and methodology for identifying mind wandering. We include figures of data validation that demonstrate the quality and temporal dynamics of the data. Finally, we describe the availability of the data and discuss potential applications including open questions for ML practitioners and examples of candidate models.</p>

<h2 id="roamm-dataset">ROAMM dataset</h2>
<h3 id="participants">Participants</h3>
<p>We recruited 58 participants from a university in the northeastern United State who were fluent in English and reported no family history of neurological disorders or epilepsy. All participants underwent screening and provided informed consent before participation. The study protocol was approved by the university Institutional Review Board. 14 participants were excluded due to issues such as equipment difficulties, incomplete experimental runs, monocular-only eye-tracking data, or missing demographic information. The final dataset includes <strong>44 participants</strong>. The participants’ age ranged from 18 to 64 years (Mean = 22.6, SD = 7.8, Median = 20, Mode = 19). This indicates a relatively young but moderately varied sample. Our sample also includes labels for gender, handedness, and self-identified ADHD.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/demographics.png" alt="demographics" style="zoom:45%;" /></p>

<h3 id="the-remind-paradigm-1">The ReMind paradigm</h3>
<p>Participants read five articles selected from Wikipedia (2015): <strong>Pluto</strong> (the dwarf planet), <strong>the Prisoner’s Dilemma</strong>, <strong>Serena Williams</strong>, <strong>the History of Film</strong>, and <strong>the Voynich Manuscript</strong>. These topics were chosen to be unfamiliar yet comprehensible without prior background knowledge. Each article was standardized by removing images and jargon, then divided into 10 pages (≈220 words per page). Pages were rendered using a custom Python script into 16 lines of black Courier-font text on a gray background. To encourage engagement and assess comprehension, we created one multiple-choice question per page. Each question was designed to require attention to that page alone.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task.png" alt="remind_task" /></p>

<p>The reading task was programmed using PsychoPy <d-cite key="Peirce2019"></d-cite>, a platform for developing psychological experiments. Each experimental session consisted of five runs, one for each article. Articles were presented in a randomized order. Before the first run, participants received task instructions and an explicit definition of mind-wandering (<em>see above figure</em>). Participants read at their own pace with no time limit per page but could not re-read previous pages. If they noticed themselves mind-wandering, they pressed the “F” key to access a dedicated reporting screen. There, they clicked on the words marking where they believed the MW episode began and ended (highlighted onscreen for clarity). If the episode began on the prior page, they marked the first word of the current page. After submitting the report, they returned to the same page to resume reading. For consistency, only one MW report was permitted per page.</p>

<h3 id="data-acquisition-and-preprocessing">Data acquisition and preprocessing</h3>
<h4 id="eye-tracking">Eye-tracking</h4>

<p>The reading task was programmed using PsychoPy <d-cite key="Peirce2019"></d-cite>, a platform for developing psychological experiments. Each experimental session consisted of five runs, one for each article. Articles were presented in a randomized order. Before the first run, participants received task instructions and an explicit definition of mind-wandering (<em>see above figure</em>). Participants read at their own pace with no time limit per page but could not re-read previous pages. If they noticed themselves mind-wandering, they pressed the “F” key to access a dedicated reporting screen. There, they clicked on the words marking where they believed the MW episode began and ended (highlighted onscreen for clarity). If the episode began on the prior page, they marked the first word of the current page. After submitting the report, they returned to the same page to resume reading. For consistency, only one MW report was permitted per page.</p>

<h3 id="data-acquisition-and-preprocessing-1">Data acquisition and preprocessing</h3>
<h4 id="eye-tracking-1">Eye-tracking</h4>
<p>The experiment was conducted in a soundproof booth to minimize distractions. We used an <strong>SR Research EyeLink 1000 Plus eye tracker</strong> to record binocular eye movements and pupil area at 1000 Hz. Each run began with calibration, repeated until EyeLink reported good accuracy (worst error &lt; 1.5°, average error &lt; 1.0°). Once calibration was complete, we recorded eye movements in sync with EEG while participants performed the reading task. PsychoPy sent page-onset triggers to both systems to keep the data streams aligned.</p>

<p>EyeLink automatically detected fixations, saccades, and blinks using default thresholds. These events were parsed into data frames, and fixations were mapped to individual words on the screen using spatial coordinates. Because pupil size data can be unreliable around blinks (due to eyelid occlusion), we corrected for this by applying linear interpolation.</p>

<h4 id="eeg">EEG</h4>
<p>We recorded simultaneous EEG using a <strong>BioSemi ActiveTwo 64-channel system</strong> at 2048 Hz. Before starting the task, we ensured all electrodes had stable connections by checking impedances and correcting any channels with unusually high values. Collected data were preprocessed in EEGLAB <d-cite key="Delorme2004"></d-cite>: resampled to 256 Hz, re-referenced, filtered (0.5–50 Hz), and bad channels interpolated. Eye and muscle artifacts were removed using independent component analysis (ICA) with standard EEGLAB parameters.</p>

<h3 id="dataset-format">Dataset format</h3>
<p>We made the ROAMM dataset easy to work with by aligning eye-tracking data to 64-channel EEG at 256 Hz. We downsampled the eye-tracking data using the real-time arrays: for each EEG time point, we identified the closest corresponding eye-tracking sample and used the pupil size at that moment. Fixations, saccades, and blinks were directly mapped using their start and end times relative to the EEG time array.</p>

<p>All data are stored in pandas DataFrames (.pkl format) for fast loading and smaller file size (compared to .csv format). Each participant has one .pkl file per run, with a total of 5 runs. Eye-tracking events like fixations, saccades, and blinks are expanded across their start-to-end times. For example, if a fixation occurs from 10 to 11 seconds, all samples within that 1-second window are annotated with <code class="language-plaintext highlighter-rouge">is_fixation = 1</code>, <code class="language-plaintext highlighter-rouge">fix_tStart = 10</code>, <code class="language-plaintext highlighter-rouge">fix_tEnd = 11</code>, <code class="language-plaintext highlighter-rouge">fix_duration = 1</code>, etc. Time stamps, page boundaries, and mind-wandering episodes are all included, along with metadata such as sampling frequency, run numbers, page numbers, and story names.</p>

<p>To maintain clarity and focus on natural reading, we defined <strong>first-pass reading</strong> as the period when participants were initially reading the text. Activities such as reading task instructions, marking mind-wandering pages, rereading after a mind-wandering report, or answering comprehension questions were excluded from this category. Each sample is labeled for first-pass reading, mind-wandering, and fixated words. Each fixated word also includes a key linking it to the original text, making it easy to generate embeddings or other vectorized representations within the context of the reading corpus for computational modeling. Additional information, including <strong>subject demographic information</strong> and <strong>comprehension question scores</strong> (with corresponding  run and page numbers), and <strong>EEG channel locations</strong> is saved in separate files for easy access.</p>

<table>
  <thead>
    <tr>
      <th>Data Category</th>
      <th>Column Count</th>
      <th>Description</th>
      <th>Example Columns</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>EEG</strong></td>
      <td>64</td>
      <td>64 electrode EEG signals</td>
      <td><code class="language-plaintext highlighter-rouge">Fp1, AF7, AF3, F1, F3...</code></td>
    </tr>
    <tr>
      <td><strong>Eye-Tracking</strong></td>
      <td>38</td>
      <td>Gaze position, pupil size, fixations, saccades, and blinks</td>
      <td><code class="language-plaintext highlighter-rouge">is_fixation, fix_eye, fix_tStart, fix_tEnd, fix_duration...</code></td>
    </tr>
    <tr>
      <td><strong>Time</strong></td>
      <td>8</td>
      <td>Timestamps, page boundaries, durations, and MW episode info</td>
      <td><code class="language-plaintext highlighter-rouge">time, page_start, page_end, page_dur, mw_onset...</code></td>
    </tr>
    <tr>
      <td><strong>Others</strong></td>
      <td>4</td>
      <td>Sampling frequency, run and page numbers, story name</td>
      <td><code class="language-plaintext highlighter-rouge">sfreq, page_num, run_num, story_name</code></td>
    </tr>
    <tr>
      <td><strong>Labels</strong></td>
      <td>4</td>
      <td>First-pass reading, MW, and fixated word annotations</td>
      <td><code class="language-plaintext highlighter-rouge">first_pass_reading, is_mw, fix_fixed_word, fix_fixed_word_key</code></td>
    </tr>
  </tbody>
</table>

<h3 id="dataset-scale">Dataset scale</h3>
<p>The ROAMM dataset is large and rich. It contains over 46 million recorded samples from 44 participants, totaling more than 50 hours of simultaneous EEG and eye-tracking data. Of these, over 26 million samples (around 30 hours) correspond to first-pass reading, which includes fixated word information at each sample. Across the 2,200 pages read, participants reported 998 mind-wandering episodes, corresponding to 45.4% of the pages. These episodes had a median duration of 5.92 seconds and a mean of 7.79 seconds and resulted in a total of 2.2 hours of reading time annotated as mind-wandering.</p>

<table>
  <thead>
    <tr>
      <th>Data Type</th>
      <th>Total Sample Count</th>
      <th>Total Time (h)</th>
      <th>Subject Avg Time (m)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Total Recording</strong></td>
      <td>46,371,584</td>
      <td>50.3</td>
      <td>68.6</td>
    </tr>
    <tr>
      <td><strong>First-Pass Reading</strong></td>
      <td>26,691,014</td>
      <td>29.0</td>
      <td>39.5</td>
    </tr>
    <tr>
      <td><strong>Mind-Wandering</strong></td>
      <td>2,045,021</td>
      <td>2.2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <td><strong>Fixation</strong></td>
      <td>38,324,700</td>
      <td>41.6</td>
      <td>56.7</td>
    </tr>
    <tr>
      <td><strong>Saccade</strong></td>
      <td>9,326,633</td>
      <td>10.1</td>
      <td>13.8</td>
    </tr>
    <tr>
      <td><strong>Blink</strong></td>
      <td>2,290,418</td>
      <td>2.5</td>
      <td>3.4</td>
    </tr>
  </tbody>
</table>

<p>The histograms below illustrate the distribution of data across participants for each attribute in the ROAMM dataset. While all participants contributed, individual differences are evident in the distributions. This highlights the real-world variability in human data and underscores the importance of carefully considering modeling approaches, whether developing a general model across participants or an individualized classifier tailored to each person.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/data_scale.png" alt="data_scale" style="zoom:20%;" /></p>

<h3 id="data-validation">Data validation</h3>
<p>We validated the ROAMM dataset to ensure high recording quality, precise alignment between EEG and eye-tracking data streams, accurate fixation-to-word mappings, and reliable labeling of MW episodes.</p>
<h4 id="eeg-and-eye-tracking-recoding-quality">EEG and eye-tracking recoding quality</h4>
<p>To assess recording quality, we inspected EEG and eye-tracking signals in parallel. As demonstrated below using a randomly selected 10-second window, preprocessed EEG from selected channels show <strong>clean activity with minimal muscle and eye artifacts</strong>. Eye-tracking features behave as expected: <strong>fixations are followed by saccades, blinks appeared distinctly</strong>, and <strong>gaze position traces reveal typical reading patterns</strong>. Specifically, x-coordinates increase left to right across each line, while y-coordinates step down across successive lines, confirming naturalistic line-by-line reading.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid.png" alt="eeg_eye_valid" /></p>

<h4 id="eeg-and-eye-tracking-alignment">EEG and eye-tracking alignment</h4>
<p>Next, we validated alignment between EEG and eye-tracking streams. Using the unfold toolbox, we deconvolved fixation-related potentials (FRPs) during periods of reading without MW. <strong>The resulting FRPs and P1 topography replicated patterns reported using the ZuCo 2.0 dataset</strong> <d-cite key="Hollenstein2019"></d-cite>, providing strong evidence for the temporal precision of our co-registered recordings.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp.png" alt="frp" /></p>

<h4 id="fixation-to-word-mapping">Fixation-to-word mapping</h4>
<p>We also validated fixation-to-word mappings by plotting gaze traces directly on reading pages. In one example, a participant read mindfully without reporting MW; in another, the same participant reported an MW episode. Onset and offset words of the MW episode were highlighted in red, while fixations appeared as colored dots. Larger dots indicated longer durations, and a purple-to-yellow gradient reflected temporal order. Fixations within MW episodes were additionally center-colored in red, and consecutive fixations were linked by red lines to mark saccades. <strong>All fixations aligned neatly with words, and the gaze traces showed clear left-to-right reading flows, confirming the accuracy of fixation-to-word mapping.</strong></p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full.png" alt="reading_page_full" /></p>

<h4 id="reliable-mw-onset">Reliable MW onset</h4>
<p>Finally, we validated MW onset labeling. In a paper currently under review, we demonstrated that incorporating MW onset information significantly improves the performance of linear regression classifiers trained to detect MW from eye-tracking features. A sliding-window analysis not only replicated prior findings of reduced fixation rates during MW episodes <d-cite key="Reichle2010"></d-cite>, but also revealed that these changes begin precisely at the reported MW onset. These findings demonstrate that the <strong>ReMind paradigm provides a powerful framework for capturing MW onset and its progression over time</strong>, ensuring that our attention state annotations are precise and grounded in reliable MW onset information.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eye_mwonset.png" alt="eye_mwonset" style="zoom:40%;" /></p>

<h3 id="data-accessibility-and-availability">Data accessibility and availability</h3>
<p>The processed datasets are publicly available on the <a href="https://osf.io/kmvgb/?view_only=688b268d5b784ff39eba5b73bc10171e">OSF</a>. Due to their large size, raw datasets are not hosted online but are available upon request. All preprocessing scripts used to generate the processed datasets are available in the <a href="https://anonymous.4open.science/r/ROAMM-6E8C/README.md">GitHub repository</a>.</p>

<h3 id="how-to-use-roamm">How to use ROAMM</h3>
<p>We put a lot of effort into making ROAMM easy to use, even if you’ve never worked with EEG or eye-tracking data before. Everything is pre-aligned and stored in <strong>pandas DataFrames</strong>, so you can load it with just a few lines of Python.</p>

<p>Once you download the dataset from <a href="https://osf.io/kmvgb/?view_only=688b268d5b784ff39eba5b73bc10171e">OSF</a>, here’s how you can get started:</p>

<p><strong>1. Import packages and set up paths</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># define data root
# this is the path to the ROAMM folder on local machine
</span><span class="n">roamm_root</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">/Users/~/ROAMM/</span><span class="sh">"</span> <span class="c1"># change this to your path
</span><span class="n">ml_data_root</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">roamm_root</span><span class="p">,</span> <span class="sh">'</span><span class="s">subject_ml_data</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<p><strong>2. Load a single run for one subject</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subject_id</span> <span class="o">=</span> <span class="sh">'</span><span class="s">s10014</span><span class="sh">'</span>
<span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
<span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s">_ml_data.pkl</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>3. Load all runs for one subject</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
    <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
    <span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_sub_all_runs</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>4. Load all subjects, filtered to first-pass reading</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load all runs for all subjects
</span><span class="n">all_subjects</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">d</span><span class="p">))]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">subject_id</span> <span class="ow">in</span> <span class="n">all_subjects</span><span class="p">:</span>
    <span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
    <span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>

    <span class="c1"># make sure each subject has 5 runs of data
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span><span class="si">}</span><span class="s"> runs instead of 5</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
        <span class="c1"># I highly recommend you to filter out reading runs that are not the first pass reading
</span>        <span class="c1"># to save memory
</span>        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># add subject id to the dataframe   
</span>        <span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">subject_id</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">subject_id</span>
        <span class="c1"># convert bool col explicitly to avoid pandas warning
</span>        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">is_blink</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_saccade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_fixation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_mw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]:</span>
            <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">==</span> <span class="bp">True</span>
        <span class="c1"># append to the dataframe
</span>        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has been loaded.</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="open-questions-for-ml-practitioners-using-roamm">Open questions for ML practitioners using ROAMM</h2>

<p>The ROAMM dataset is rich in scope, combining multiple valuable modalities: <strong>eye-tracking</strong> (gaze position and pupil size), <strong>brain signals</strong> (i.e., EEG), <strong>human attention states</strong>, and <strong>linguistic content</strong> (the reading text itself). This multimodal design provides countless opportunities for machine learning practitioners to explore how these signals interact. Below, we highlight 4 open questions that showcase the potential of ROAMM for advancing both cognitive science and computational modeling.</p>

<p><img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/roamm_modalities.png" alt="roamm_modalities" style="zoom:40%;" /></p>

<h3 id="learn-shared-representation-from-eeg-and-eye-tracking">Learn shared representation from EEG and eye-tracking</h3>
<blockquote>
   	<b>The eyes are the windows to the soul.</b>
    											— William Shakespeare, “King Richard III” Act V, Sc.3
</blockquote>

<p>Eyes, particularly pupil size, reveal much about internal cognitive states and arousal <d-cite key="Castellotti2025"></d-cite>. With a dataset at the scale of ROAMM, we can now ask whether eye-tracking and EEG share common patterns of variance that allow them to be tightly linked.</p>

<p>One way to explore this question is to use the established multi-modal embedding method like CLIP <d-cite key="Radford2021"></d-cite>, which was originally applied to learn joint representations of images and text via contrastive learning. A <strong>CLIP-style</strong> model could be trained on ROAMM by contrasting matched and mismatched EEG and eye-tracking segments. If two modalities indeed have features that share significant variance, the resulting embeddings would enable accurate cross-modal classification. Beyond classification, shared representation opens the door to conditional decoding from one modality to the other. For example, one can train a <strong>diffusion transformer model (DiT)</strong> <d-cite key="Peebles2022"></d-cite> to reconstruct a subject’s EEG signals using eye-tracking data as a generative prior. Since there exists a shared representation space of EEG and eye-tracking, the DiT can fuse the information from eye-tracking data through cross-attention.</p>

<h3 id="build-a-momentary-human-attention-decoder">Build a momentary human attention decoder</h3>
<p>EEG and eye-tracking are not the only modalities in ROAMM that may exhibit strong associations. Previous studies have trained models to detect attention states from EEG and eye-tracking separately. We hypothesize that, when combined together, they can provide a more reliable estimate of a subject’s attention state during reading.</p>

<p>To evaluate this hypothesis, one could train simple classifiers such as regression models, SVM, gradient boosting, or neural models that takes the shared representations of EEG and eye-tracking at a given moment to predict the subject’s attention state at that same time step. However, attention is not merely a transient experience; it unfolds dynamically over time. Thus, signals from a single moment are unlikely to provide sufficient information for accurate prediction. To capture attention’s temporal dynamics, one can train sequential models like <strong>long short-term memory networks</strong>, <strong>temporal convolutional networks</strong>, or <strong>generative transformers</strong>, which have demonstrated superior performance in modeling text data. These models can be trained to predict future attention states from EEG, eye-tracking, and attention states from both current and prior windows, enabling moment-to-moment attention decoding.</p>

<h3 id="is-human-attention-what-we-need-for-neural-decoding">Is human attention what we need for neural decoding?</h3>
<p>Decoding neural signals has become a popular topic, with several studies attempting to use EEG to decode text <d-cite key="Liu2024, Wang2024"></d-cite>. However, their main focus was on the attention mechanisms in transformers, but they largely neglected fluctuations in human attention during the task. This can be problematic: during mind-wandering, readers still maintain the outward behavior of reading, moving their eyes from left to right and line by line, but their visual input is disrupted by internal thoughts. Cognitive resources that should be allocated to word recognition and comprehension are instead consumed by spontaneous mental activity. In other words, the brain itself cannot fully follow what the eyes are reading during mind-wandering. This raises a fascinating question: <strong>does knowledge of human attention states improve neural decoding performance?</strong></p>

<p>A straightforward way to address this question is to train an EEG2text decoder separately on data segments from normal reading versus mind-wandering. <strong>Performance differences between these conditions would reveal whether filtering out periods of “mindless reading” provides a decoding advantage.</strong> If we observe better performance when training only on attentive reading, the result would be intuitive: how could a model recover information that the brain itself fails to process?</p>

<p>However, the more intriguing possibility is if decoding performance remains similar regardless of attention state. This would suggest that information about the text is encoded at lower levels of the visual or sensory hierarchy. In such a scenario, the model may be able to extract signals from early visual or pre-attentive neural activity that are not available to conscious awareness. This opens up provocative implications: <strong>machine learning models could potentially reveal implicit or subliminal processing of linguistic information in the brain.</strong> In other words, this is not a mind-reader but something at another level: a <strong>subconscious-reader</strong> that can uncover information from your brain even when you are not aware. It’s as if your neurons are whispering secrets that only the model can hear.</p>

<p>Thus, testing whether attention modulates neural decoding performance not only has practical implications for building better brain–computer interfaces, but also addresses fundamental cognitive neuroscience questions about the boundary between unconscious encoding and conscious comprehension.</p>

<h3 id="use-eeg-eye-tracking-human-attention-and-reading-text-to-predict-comprehension">Use EEG, eye-tracking, human attention, and reading text to predict comprehension</h3>
<p>The previous questions focused on the link between pairs of modalities in our dataset. But as the saying goes, <em>“only children make choices, adults want it all!”</em> For machine learning experts who are not satisfied with pairwise associations and eager to showcase the full power of multimodal modeling, the next challenge is to use all available modalities from ROAMM: EEG, eye-tracking, attention states, and reading text itself. The task we propose is to predict reading comprehension, using ROAMM’s page-level comprehension scores. While page-level labels are coarse and may not perfectly reflect moment-to-moment understanding, they still provide a valuable proxy for comprehension that can anchor multimodal learning.</p>

<p>This problem requires complex model architectures and training frameworks that can integrate heterogeneous data streams. One can opt for the <strong>traditional fusion methods like early fusion</strong> in which features from all modalities are concatenated and processed jointly within a single parameterized model. Although considered a traditional technique, early fusion remains prevalent in recent large-scale multimodal systems for text and images (e.g., <d-cite key="ChameleonTeam2024, Lin2024"></d-cite>) (early fusion via concatenation, late fusion via ensembling, or hybrid fusion). Besides fusion, we can also train individual models to embed each modality. Examples of this <strong>late fusion include CLIP and Imagine Bind</strong> <d-cite key="Radford2021, Girdhar2023"></d-cite> which trains transformer encoders to map multi-modal data across into an embedding space. Downstream tasks, such as comprehension prediction, can be done by training lightweight classification on top of the shared embeddings. When applying late fusion on ROAMM, one can follow the existing practice, using the same architecture to embed data from all modalities. Alternatively, they can use specific architecture with inductive bias that accommodates the invariance present in the data (e.g., graph neural network (GNN) for EEG data <d-cite key="Klepl2023"></d-cite>).</p>

<p>A model that successfully predicts comprehension from this rich multimodal space would not only advance cognitive modeling, but also contribute to the emerging field of Foundation Models for the Brain and Body (<em>Yes, another workshop hosted this year</em>). By integrating physiological, behavioral, and linguistic signals into a single predictive framework, we move closer to general-purpose models of human cognition. A recent Nature study <d-cite key="Binz2025"></d-cite> showes that large-scale multimodal learning can capture human behavior across a wide range of domains. Extending these ideas to ROAMM provides an opportunity to build neurocognitive foundation models during naturalistic reading environments.</p>

<h2 id="conclusions">Conclusions</h2>

<p>In this work, we introduced the <strong>Reading Observed At Mindless Moments (ROAMM)</strong> dataset, a large-scale, multimodal resource capturing simultaneous EEG and eye-tracking data during naturalistic reading. By using ReMind paradigm, ROAMM stands out among existing reading datasets by providing a highly naturalistic reading environment, temporally-resolved attention labels, and precise alignment between neural and behavioral signals.</p>

<p>We provided an overview of the dataset’s acquisition, preprocessing, and structure, highlighting its <strong>scale, richness, and quality</strong>. Validation analyses confirmed <strong>high-fidelity recordings</strong>, <strong>accurate fixation-to-word mappings</strong>, and <strong>reliable labeling of mind-wandering episodes</strong>, making ROAMM suitable for rigorous cognitive and computational modeling.</p>

<p>Beyond describing the dataset, we outlined a set of open questions that illustrate its potential for advancing both neuroscience and machine learning. These include 1) learning shared representations between EEG and eye-tracking, 2) building moment-to-moment attention decoders, 3) investigating the role of attention in neural decoding, and 4) predicting reading comprehension using fully multimodal data. ROAMM thus provides a unique opportunity to explore the interactions between brain, eye movements, attention, and language, enabling development of models that better reflect real human cognition.</p>

<p>In summary, ROAMM not only offers a rich resource for fundamental research on attention and reading but also serves as a platform for developing advanced multimodal machine learning models. By bridging cognitive science and computational modeling, it paves the way toward neurocognitive foundation models capable of capturing complex and naturalistic human behavior.</p>]]></content><author><name>Haorui Sun</name></author><summary type="html"><![CDATA[ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes to advance research in attention, language, and machine learning.]]></summary></entry><entry><title type="html">Distill Example3</title><link href="https://data-brain-mind.github.io/tutorials/distill-example3/" rel="alternate" type="text/html" title="Distill Example3" /><published>2025-09-08T00:00:00+08:00</published><updated>2025-09-08T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example3</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example3/"><![CDATA[]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://data-brain-mind.github.io/tutorials/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post" /><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> 
that brought a significant improvement to the loading and rendering speed, which is now 
<a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<h2 id="images-and-figures">Images and Figures</h2>

<p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
To include images in your submission in this way, you must do something like the following:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div>

<p>which results in the following image:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory
<code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p>

<p>Please avoid using the direct markdown method of embedding images; they may not be properly resized.
Some more complex ways to load images (note the different styles of the shapes/shadows):</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="interactive-figures">Interactive Figures</h3>

<p>Here’s how you could embed interactive figures that have been exported as HTML files.
Note that we will be using plotly for this demo, but anything built off of HTML should work
(<strong>no extra javascript is allowed!</strong>).
All that’s required is for you to export your figure into HTML format, and make sure that the file
exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory.
To embed it into any page, simply insert the following code anywhere into your page.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div>

<p>For example, the following code can be used to generate the figure underneath it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>And then include it with the following:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div>

<p>Voila!</p>

<div class="l-page">
  <iframe src="/tutorials/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe>
</div>

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
It supports more than 100 languages.
This example is in C++.
All you have to do is wrap your code in a liquid tag:</p>

<p>{% highlight c++ linenos %}  <br /> code code code <br /> {% endhighlight %}</p>

<p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="diagrams">Diagrams</h2>

<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin.
Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p>

<p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p>

<p><strong>Note:</strong> This is not supported for local rendering!</p>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div>

<div class="jekyll-diagrams diagrams mermaid">
  <svg id="mermaid-1763940943186" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1763940943186 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1763940943186 .node circle,#mermaid-1763940943186 .node ellipse,#mermaid-1763940943186 .node polygon,#mermaid-1763940943186 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1763940943186 .node.clickable{cursor:pointer}#mermaid-1763940943186 .arrowheadPath{fill:#333}#mermaid-1763940943186 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1763940943186 .edgeLabel{background-color:#e8e8e8}#mermaid-1763940943186 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1763940943186 .cluster text{fill:#333}#mermaid-1763940943186 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1763940943186 .actor{stroke:#ccf;fill:#ececff}#mermaid-1763940943186 text.actor{fill:#000;stroke:none}#mermaid-1763940943186 .actor-line{stroke:grey}#mermaid-1763940943186 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1763940943186 .messageLine0,#mermaid-1763940943186 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1763940943186 #arrowhead{fill:#333}#mermaid-1763940943186 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1763940943186 .messageText{fill:#333;stroke:none}#mermaid-1763940943186 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1763940943186 .labelText,#mermaid-1763940943186 .loopText{fill:#000;stroke:none}#mermaid-1763940943186 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1763940943186 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1763940943186 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1763940943186 .section{stroke:none;opacity:.2}#mermaid-1763940943186 .section0{fill:rgba(102,102,255,.49)}#mermaid-1763940943186 .section2{fill:#fff400}#mermaid-1763940943186 .section1,#mermaid-1763940943186 .section3{fill:#fff;opacity:.2}#mermaid-1763940943186 .sectionTitle0,#mermaid-1763940943186 .sectionTitle1,#mermaid-1763940943186 .sectionTitle2,#mermaid-1763940943186 .sectionTitle3{fill:#333}#mermaid-1763940943186 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1763940943186 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1763940943186 .grid path{stroke-width:0}#mermaid-1763940943186 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1763940943186 .task{stroke-width:2}#mermaid-1763940943186 .taskText{text-anchor:middle;font-size:11px}#mermaid-1763940943186 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1763940943186 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1763940943186 .taskText0,#mermaid-1763940943186 .taskText1,#mermaid-1763940943186 .taskText2,#mermaid-1763940943186 .taskText3{fill:#fff}#mermaid-1763940943186 .task0,#mermaid-1763940943186 .task1,#mermaid-1763940943186 .task2,#mermaid-1763940943186 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1763940943186 .taskTextOutside0,#mermaid-1763940943186 .taskTextOutside1,#mermaid-1763940943186 .taskTextOutside2,#mermaid-1763940943186 .taskTextOutside3{fill:#000}#mermaid-1763940943186 .active0,#mermaid-1763940943186 .active1,#mermaid-1763940943186 .active2,#mermaid-1763940943186 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1763940943186 .activeText0,#mermaid-1763940943186 .activeText1,#mermaid-1763940943186 .activeText2,#mermaid-1763940943186 .activeText3{fill:#000!important}#mermaid-1763940943186 .done0,#mermaid-1763940943186 .done1,#mermaid-1763940943186 .done2,#mermaid-1763940943186 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1763940943186 .doneText0,#mermaid-1763940943186 .doneText1,#mermaid-1763940943186 .doneText2,#mermaid-1763940943186 .doneText3{fill:#000!important}#mermaid-1763940943186 .crit0,#mermaid-1763940943186 .crit1,#mermaid-1763940943186 .crit2,#mermaid-1763940943186 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1763940943186 .activeCrit0,#mermaid-1763940943186 .activeCrit1,#mermaid-1763940943186 .activeCrit2,#mermaid-1763940943186 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1763940943186 .doneCrit0,#mermaid-1763940943186 .doneCrit1,#mermaid-1763940943186 .doneCrit2,#mermaid-1763940943186 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1763940943186 .activeCritText0,#mermaid-1763940943186 .activeCritText1,#mermaid-1763940943186 .activeCritText2,#mermaid-1763940943186 .activeCritText3,#mermaid-1763940943186 .doneCritText0,#mermaid-1763940943186 .doneCritText1,#mermaid-1763940943186 .doneCritText2,#mermaid-1763940943186 .doneCritText3{fill:#000!important}#mermaid-1763940943186 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1763940943186 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1763940943186 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1763940943186 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1763940943186 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1763940943186 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1763940943186 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1763940943186 #compositionEnd,#mermaid-1763940943186 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1763940943186 #aggregationEnd,#mermaid-1763940943186 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1763940943186 #dependencyEnd,#mermaid-1763940943186 #dependencyStart,#mermaid-1763940943186 #extensionEnd,#mermaid-1763940943186 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1763940943186 .branch-label,#mermaid-1763940943186 .commit-id,#mermaid-1763940943186 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1763940943186 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>

<hr />

<h2 id="tweets">Tweets</h2>

<p>An example of displaying a tweet:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p>An example of pulling from a timeline:</p>
<div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p>

<hr />

<h2 id="blockquotes">Blockquotes</h2>

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
    <ul>
      <li>Unordered sub-list.</li>
    </ul>
  </li>
  <li>Actual numbers don’t matter, just that it’s a number
    <ol>
      <li>Ordered sub-list</li>
    </ol>
  </li>
  <li>
    <p>And another item.</p>

    <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

    <p>To have a line break without a paragraph, you will need to use two trailing spaces.
Note that this line is separate, but within the same paragraph.
(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p>
  </li>
</ol>

<ul>
  <li>Unordered lists can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes 
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the 
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://data-brain-mind.github.io/tutorials/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)" /><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example2</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example2/"><![CDATA[<p>
  This is a sample blog post written in HTML (while the other <a href="/tutorials/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead.
</p>

<p>
  Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.
</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph.
Here is an example:
$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$
</p>

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> 
that brought a significant improvement to the loading and rendering speed, which is now 
<a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<h2 id="images-and-figures">Images and Figures</h2>

<p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
You can display images from this repository using the following code:</p>

<pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre>

<p>which results in the following image:</p>

<figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/iclr.png"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>



<p>
  To ensure that there are no namespace conflicts, you must save your asset to your unique directory
  `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission.  
</p>

<p>
  Please avoid using the direct HTML method of embedding images; they may not be properly resized.
  Some below complex ways to load images (note the different styles of the shapes/shadows):  
</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/9.jpg"
      class="img-fluid rounded z-depth-1"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg"
      class="img-fluid rounded z-depth-1"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/8.jpg"
      class="img-fluid z-depth-2"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/10.jpg"
      class="img-fluid z-depth-2"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/11.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/12.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>

<h3>Interactive Figures</h3>

<p>
  Here's how you could embed interactive figures that have been exported as HTML files.
  Note that we will be using plotly for this demo, but anything built off of HTML should work.
  All that's required is for you to export your figure into HTML format, and make sure that the file
  exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory.
  To embed it into any page, simply insert the following code anywhere into your page.  
</p>

<pre><code>{% include [FIGURE_NAME].html %}</code></pre>

<p>
For example, the following code can be used to generate the figure underneath it.
</p>

<pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre>

And then include it with the following:

<pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre>

Voila!

<div class="l-page">
  <iframe src="/tutorials/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe>
</div>


<h2 id="citations">Citations</h2>


<p>
  Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag.
    The key attribute is a reference to the id provided in the bibliography.
    The key attribute can take multiple ids, separated by commas.    
</p>

<p>
  The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
  If you have an appendix, a bibliography is automatically created and populated in it.  
</p>

<p>
  Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
  However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work.  
</p>


<h2 id="footnotes">Footnotes</h2>

<p>
  Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag.
    The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>
</p>


<h2 id="code-blocks">Code Blocks</h2>

<p>
  This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
  It supports more than 100 languages.
  This example is in C++.
  All you have to do is wrap your code in a liquid tag as follows:  
</p>

<pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre>

The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below:

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>



<h2 id="diagrams">Diagrams</h2>

<p>
  This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin.
  Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc.  
</p>

<p>
  <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine.
  Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW.
  For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README.  
</p>

<p>
  <b>Note:</b> This is not supported for local rendering!
</p>

<p>
  The diagram below was generated by the following code:
</p>

<pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre>

<div class='jekyll-diagrams diagrams mermaid'>
  <svg id="mermaid-1763940943816" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1763940943816 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1763940943816 .node circle,#mermaid-1763940943816 .node ellipse,#mermaid-1763940943816 .node polygon,#mermaid-1763940943816 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1763940943816 .node.clickable{cursor:pointer}#mermaid-1763940943816 .arrowheadPath{fill:#333}#mermaid-1763940943816 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1763940943816 .edgeLabel{background-color:#e8e8e8}#mermaid-1763940943816 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1763940943816 .cluster text{fill:#333}#mermaid-1763940943816 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1763940943816 .actor{stroke:#ccf;fill:#ececff}#mermaid-1763940943816 text.actor{fill:#000;stroke:none}#mermaid-1763940943816 .actor-line{stroke:grey}#mermaid-1763940943816 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1763940943816 .messageLine0,#mermaid-1763940943816 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1763940943816 #arrowhead{fill:#333}#mermaid-1763940943816 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1763940943816 .messageText{fill:#333;stroke:none}#mermaid-1763940943816 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1763940943816 .labelText,#mermaid-1763940943816 .loopText{fill:#000;stroke:none}#mermaid-1763940943816 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1763940943816 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1763940943816 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1763940943816 .section{stroke:none;opacity:.2}#mermaid-1763940943816 .section0{fill:rgba(102,102,255,.49)}#mermaid-1763940943816 .section2{fill:#fff400}#mermaid-1763940943816 .section1,#mermaid-1763940943816 .section3{fill:#fff;opacity:.2}#mermaid-1763940943816 .sectionTitle0,#mermaid-1763940943816 .sectionTitle1,#mermaid-1763940943816 .sectionTitle2,#mermaid-1763940943816 .sectionTitle3{fill:#333}#mermaid-1763940943816 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1763940943816 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1763940943816 .grid path{stroke-width:0}#mermaid-1763940943816 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1763940943816 .task{stroke-width:2}#mermaid-1763940943816 .taskText{text-anchor:middle;font-size:11px}#mermaid-1763940943816 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1763940943816 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1763940943816 .taskText0,#mermaid-1763940943816 .taskText1,#mermaid-1763940943816 .taskText2,#mermaid-1763940943816 .taskText3{fill:#fff}#mermaid-1763940943816 .task0,#mermaid-1763940943816 .task1,#mermaid-1763940943816 .task2,#mermaid-1763940943816 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1763940943816 .taskTextOutside0,#mermaid-1763940943816 .taskTextOutside1,#mermaid-1763940943816 .taskTextOutside2,#mermaid-1763940943816 .taskTextOutside3{fill:#000}#mermaid-1763940943816 .active0,#mermaid-1763940943816 .active1,#mermaid-1763940943816 .active2,#mermaid-1763940943816 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1763940943816 .activeText0,#mermaid-1763940943816 .activeText1,#mermaid-1763940943816 .activeText2,#mermaid-1763940943816 .activeText3{fill:#000!important}#mermaid-1763940943816 .done0,#mermaid-1763940943816 .done1,#mermaid-1763940943816 .done2,#mermaid-1763940943816 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1763940943816 .doneText0,#mermaid-1763940943816 .doneText1,#mermaid-1763940943816 .doneText2,#mermaid-1763940943816 .doneText3{fill:#000!important}#mermaid-1763940943816 .crit0,#mermaid-1763940943816 .crit1,#mermaid-1763940943816 .crit2,#mermaid-1763940943816 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1763940943816 .activeCrit0,#mermaid-1763940943816 .activeCrit1,#mermaid-1763940943816 .activeCrit2,#mermaid-1763940943816 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1763940943816 .doneCrit0,#mermaid-1763940943816 .doneCrit1,#mermaid-1763940943816 .doneCrit2,#mermaid-1763940943816 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1763940943816 .activeCritText0,#mermaid-1763940943816 .activeCritText1,#mermaid-1763940943816 .activeCritText2,#mermaid-1763940943816 .activeCritText3,#mermaid-1763940943816 .doneCritText0,#mermaid-1763940943816 .doneCritText1,#mermaid-1763940943816 .doneCritText2,#mermaid-1763940943816 .doneCritText3{fill:#000!important}#mermaid-1763940943816 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1763940943816 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1763940943816 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1763940943816 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1763940943816 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1763940943816 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1763940943816 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1763940943816 #compositionEnd,#mermaid-1763940943816 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1763940943816 #aggregationEnd,#mermaid-1763940943816 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1763940943816 #dependencyEnd,#mermaid-1763940943816 #dependencyStart,#mermaid-1763940943816 #extensionEnd,#mermaid-1763940943816 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1763940943816 .branch-label,#mermaid-1763940943816 .commit-id,#mermaid-1763940943816 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1763940943816 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>



<h2 id="tweets">Tweets</h2>

<p>
  An example of displaying a tweet:
  <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>  
</p>

<p>
  An example of pulling from a timeline:
  <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>  
</p>

<p>
  For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> 
</p>


<h2 id="blockquotes">Blockquotes</h2>

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote>


<h2 id="layouts">Layouts</h2>

The main text column is referred to as the body.
It's the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you'll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>


<h2 id="other-typography">Other Typography?</h2>

<p>
  Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>.
</p>

<p>
  Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>.
</p>

<p>
  Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s>
</p>

<ul>
  <li>First ordered list item</li>
  <li>Another item</li>
  <ol>
    <li>Unordered sub-list. </li>
  </ol>
  <li>And another item.</li>
</ul>



<p>
  For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code.
</p>

<pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre>
 
<pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre>

<pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre>

<p>
  A table can be created with the <code>&lt;table&gt;</code> element. Below is an example
</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>


<p>
  <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote>
</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>